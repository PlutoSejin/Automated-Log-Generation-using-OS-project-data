{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hotBhcGAVr8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV23QwdwybbV"
      },
      "outputs": [],
      "source": [
        "# 실행하고 링크 누르기\n",
        "# 런타임 유형 v2-8 TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSNK-FnUWdSi",
        "outputId": "0a48c63d-3a34-43e1-ef67-c4019a3f134a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up GCS access...\n"
          ]
        }
      ],
      "source": [
        "print(\"Setting up GCS access...\")\n",
        "import os\n",
        "os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afR7oxX3xZSx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSXMeKCsnACN",
        "outputId": "35cacb11-808d-4ef0-ba46-fb5571af6c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n"
          ]
        }
      ],
      "source": [
        "##버전은 언제든지 변경이 될 수 있음. 라이브러리 문제. 아래 cell에서 import 되지 않으면 맞는 버전 찾기\n",
        "from IPython.display import clear_output\n",
        "!pip install gcsfs\n",
        "!pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.15.0/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "!pip install tensorflow\n",
        "!pip install t5\n",
        "#!pip install 'jax[tpu]==0.4.26' -f https://storage.googleapis.com/jax-releases/libtpu_releases.html -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "#!pip install jaxlib==0.4.26\n",
        "!pip install tensorflow-gcs-config==2.15.0\n",
        "!pip install -q tensorflow-text==2.15.0\n",
        "clear_output()\n",
        "print(\"Installing dependencies...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vznWDjUnpBIL"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import os\n",
        "import gin\n",
        "from contextlib import contextmanager\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import logging as py_logging\n",
        "import t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_scbA0x9rX6",
        "outputId": "52993845-dd97-4b87-9d81-17539aaa84d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x7d00144f97b0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.app.flags.DEFINE_string ('f', '', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6xQTS2oXLBB",
        "outputId": "117d6a64-e8a0-4584-face-5f6e5ee76765"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/device:TPU:0', device_type='TPU'), LogicalDevice(name='/device:TPU:1', device_type='TPU'), LogicalDevice(name='/device:TPU:2', device_type='TPU'), LogicalDevice(name='/device:TPU:3', device_type='TPU'), LogicalDevice(name='/device:TPU:4', device_type='TPU'), LogicalDevice(name='/device:TPU:5', device_type='TPU'), LogicalDevice(name='/device:TPU:6', device_type='TPU'), LogicalDevice(name='/device:TPU:7', device_type='TPU')]\n",
            "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, -5315752443649472665), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 17179869184, 27174686991857553), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 17179869184, -2330879005499457346), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 17179869184, -8475818048530131393), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 17179869184, -7821953537311121992), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 17179869184, 4334979118966232378), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 17179869184, -5030812758612352774), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 17179869184, -8990364639213490880), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4551452486398796834), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, -433225702783494080)]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_gcs_config\n",
        "import tensorflow.compat.v1 as tf\n",
        "TPU_TOPOLOGY = \"2x2\"\n",
        "\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')  # TPU detection\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.config.experimental.list_logical_devices('TPU')\n",
        "    tf.config.experimental.list_physical_devices('TPU')\n",
        "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "    s = tf.Session()\n",
        "    print(s.list_devices())\n",
        "    tensorflow_gcs_config.configure_gcs_from_colab_auth(tf.config.list_logical_devices('TPU')[1])\n",
        "except ValueError:\n",
        "    raise BaseException(\n",
        "        'ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!'\n",
        "    )\n",
        "\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "#LOGGING\n",
        "tf.get_logger().propagate = False\n",
        "py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeYlWB1-YLPK"
      },
      "outputs": [],
      "source": [
        "representation = \"custom\" #@param ['tokens','custom'] #tokens은 leonid, custom은 우리가 만든 데이터셋\n",
        "pretraining_task = \"word_real\" #@param ['token_real','word_real'] #word_real은 word 기준인 pretrained model 경로, token_real은 token 기준인 pretrined model 경로\n",
        "scheduler = \"isr\" #@param [\"polynomial\", \"constant\", \"isr\", \"slanted\"]\n",
        "my_finetuning_task = \"multi-log-injection\" #@param ['multi-log-injection', 'single-log-injection', 'classifier'] #multi-log injection은 다중 로그 삽입, classifier은 로그 삽입 여부\n",
        "multi_log_injection_type = \"one-to-n\" #@param [ \"one-to-n\"]\n",
        "s_TOP = 'TOP-5' #@param['TOP-1','TOP-3','TOP-5']\n",
        "m_TOP = 'TOP5' #@param['TOP1','TOP3','TOP5']\n",
        "prefix = 'MULTI_LOG_INJECTION: ' #@param ['SINGLE_LOG_INJECTION: ', 'CLASSIFICATION: ', 'MULTI_LOG_INJECTION: ']\n",
        "TASK_NAME = \"log_injection\" #@param ['classification', 'log_injection'] #log_injection은 로그 삽입 메세지 생성, classification은 로그 삽입 여부 결정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KmddevxlYNim",
        "outputId": "2af97dbe-45ba-40fc-d517-7be837bebcb8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gs://log_gen/finetune/multi-log-injection/one-to-n/custom/TOP5/test_add_token.tsv'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if my_finetuning_task == \"multi-log-injection\":\n",
        "  train_path = f'gs://log_gen/finetune/{my_finetuning_task}/{multi_log_injection_type}/{representation}/{m_TOP}/train_add_token.tsv'\n",
        "  eval_path = f'gs://log_gen/finetune/{my_finetuning_task}/{multi_log_injection_type}/{representation}/{m_TOP}/eval_add_token.tsv'\n",
        "  test_path = f'gs://log_gen/finetune/{my_finetuning_task}/{multi_log_injection_type}/{representation}/{m_TOP}/test_add_token.tsv'\n",
        "elif my_finetuning_task ==\"single-log-injection\":\n",
        "  train_path = f'gs://log_gen/finetune/{my_finetuning_task}/{representation}/{s_TOP}/train.tsv'\n",
        "  eval_path = f'gs://log_gen/finetune/{my_finetuning_task}/{representation}/{s_TOP}/eval.tsv'\n",
        "  test_path = f'gs://log_gen/finetune/{my_finetuning_task}/{representation}/{s_TOP}/test.tsv'\n",
        "else:\n",
        "  train_path = f'gs://log_gen/finetune/{my_finetuning_task}/train.tsv'\n",
        "  eval_path = f'gs://log_gen/finetune/{my_finetuning_task}/eval.tsv'\n",
        "  test_path = f'gs://log_gen/finetune/{my_finetuning_task}/test-75-25.tsv'\n",
        "\n",
        "finetune_datasets_paths = {\n",
        "    \"train\":      train_path,\n",
        "    \"validation\": eval_path,\n",
        "    \"test\": test_path\n",
        "}\n",
        "train_path\n",
        "eval_path\n",
        "test_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "noc8kTnlZDie",
        "outputId": "60deee62-cc06-4fe0-a28d-611599153c15"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Storage paths\n",
        "if my_finetuning_task == \"multi-log-injection\":\n",
        "  FINETUNE_MODEL_DIR = f\"gs://log_gen/finetuned-model/{my_finetuning_task}/{multi_log_injection_type}/{representation}/word_fixed\" #WORD PRETRAIN 모델을 사용하면 word_fixed, TOKEN PRETRAIN 모델을 사용하면 token_fixed\n",
        "elif my_finetuning_task == \"single-log-injection\":\n",
        "  FINETUNE_MODEL_DIR = f\"gs://log_gen/finetuned-model/{my_finetuning_task}/{representation}/token\"\n",
        "else:\n",
        "  FINETUNE_MODEL_DIR = f\"gs://log_gen/finetuned-model/{my_finetuning_task}/token/\"\n",
        "\n",
        "PRETRAIN_MODEL_DIR= f'gs://log_gen/pretrained-model/custom/{pretraining_task}'\n",
        "FINETUNE_MODEL_DIR\n",
        "#PRETRAIN_MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A43RCJUWhyUc"
      },
      "outputs": [],
      "source": [
        "vocab_model_path = f\"gs://log_gen/tokenizer/tokens/tokenizer.model\"\n",
        "vocab_path = f\"gs://log_gen/tokenizer/tokens/tokenizer.vocab\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RS83UyOixAa"
      },
      "outputs": [],
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "num_special_mask_tokens = 100 #@param {type: \"integer\"}\n",
        "\n",
        "def load_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, num_special_mask_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgmYoJEQvjsV",
        "outputId": "8047e24a-5ce7-4a68-ff18-2426d187e406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://log_gen/learning_rate_scheduler/isr\n",
            "Copying gs://log_gen/learning_rate_scheduler/isr/isr.gin...\n",
            "/ [1/1 files][ 11.5 KiB/ 11.5 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/11.5 KiB.                                     \n"
          ]
        }
      ],
      "source": [
        "BASE_DIR = f\"gs://log_gen/learning_rate_scheduler/{scheduler}\"\n",
        "print(BASE_DIR)\n",
        "GIN_FILENAME = \"isr.gin\" #@param {type : \"polynomial\",\"isr\"}#\n",
        "# 'operative_config.gin'\n",
        "# 'no_pretraining_operative_config.gin'\n",
        "remote_gin_path = os.path.join(BASE_DIR, GIN_FILENAME)\n",
        "LOCAL_GIN_PATH = f\"/content/{GIN_FILENAME}\"\n",
        "!gsutil -m cp $remote_gin_path $LOCAL_GIN_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur3JCVfGD-DA"
      },
      "outputs": [],
      "source": [
        "\"\"\"AttributeError                            Traceback (most recent call last)\n",
        "<ipython-input-13-efe9cf962c6f> in <cell line: 1>()\n",
        "      1 with gin.unlock_config():\n",
        "----> 2     gin.parse_config_file(LOCAL_GIN_PATH)\n",
        "\n",
        "10 frames\n",
        "/usr/local/lib/python3.10/dist-packages/mesh_tensorflow/tpu_variables.py in <module>\n",
        "    224\n",
        "    225\n",
        "--> 226 ops.register_tensor_conversion_function(ReplicatedVariable, _tensor_conversion)\n",
        "    227\n",
        "    228 if not TF_23:\n",
        "\n",
        "AttributeError: module 'tensorflow.python.framework.ops' has no attribute 'register_tensor_conversion_function'\n",
        "  In file \"/content/isr.gin\", line 1\n",
        "    import mesh_tensorflow.optimize\"\"\"\n",
        "#위와 같은 에러가 아래 cell에서 발생 가능\n",
        "\n",
        "# 에러에서 \"/usr/local/lib/python3.10/dist-packages/mesh_tensorflow/tpu_variables.py/usr/local/lib/python3.10/dist-packages/mesh_tensorflow/tpu_variables.py\"를 누르면 tpu_variable.py가 옆에 나옴\n",
        "# tpu_variable.py 226줄에서 \"ops.register_tensor_conversion_function(ReplicatedVariable, _tensor_conversion)\" 를\n",
        "# ops.tensor_conversion_registry.register_tensor_conversion_function(ReplicatedVariable, _tensor_conversion) 로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRxbWXnRmgvT"
      },
      "outputs": [],
      "source": [
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(LOCAL_GIN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "918WKqVojYjU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps5zKDcAwr5g"
      },
      "outputs": [],
      "source": [
        "def load_dataset(split, shuffle_files=True):\n",
        "  del shuffle_files\n",
        "\n",
        "  ds = tf.data.TextLineDataset(finetune_datasets_paths[split])\n",
        "  #ds = ds.skip(42)\n",
        "  def has_two_fields(line):\n",
        "        return tf.equal(tf.size(tf.strings.split([line], sep=\"\\t\")), 2)\n",
        "\n",
        "  ds = ds.filter(has_two_fields)\n",
        "  ds = ds.map(functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  def truncate_examples(ex):\n",
        "        ex['input'] = tf.strings.substr(ex['input'], 0, 2048)\n",
        "        ex['output'] = tf.strings.substr(ex['output'], 0, 2048)\n",
        "        return ex\n",
        "\n",
        "  ds = ds.map(truncate_examples)\n",
        "\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H1pmCnZmlHM",
        "outputId": "58ebedc0-91c8-4730-a154-1821f192317b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'augmented_input', 'output': b'original_method'}\n",
            "{'input': b'\"protected String[] listSupportedSASLMechanisms(AtomicBoolean done) { final CountDownLatch listMechsLatch = new CountDownLatch(1); final AtomicReference<String> supportedMechs = new AtomicReference<String>(); Operation listMechsOp = opFact.saslMechs(new OperationCallback() { @Override public void receivedStatus(OperationStatus status) { if(status.isSuccess()) { supportedMechs.set(status.getMessage()); getLogger().debug(\"\"Received SASL supported mechs: \"\" + status.getMessage()); } else { getLogger().warn(\"\"Received non-success response for SASL mechs: \"\" + status); } } @Override public void complete() { listMechsLatch.countDown(); } }); conn.insertOperation(node, listMechsOp); try { if (!conn.isShutDown()) { listMechsLatch.await(); } else { done.set(true); // Connection is shutting down, tear.down. } } catch(InterruptedException ex) { getLogger().warn(\"\"Interrupted in Auth while waiting for SASL mechs.\"\"); // we can be interrupted if we were in the // process of auth\\'ing and the connection is // lost or dropped due to bad auth Thread.currentThread().interrupt(); if (listMechsOp != null) { listMechsOp.cancel(); } done.set(true); // If we were interrupted, tear down. } String supported = supportedMechs.get(); if (supported == null || supported.isEmpty()) { return null; } return supported.split(MECH_SEPARATOR); }<log_message> \"\"Authenticated to \"\" </log_message>  <similarity> 0.58 </similarity><log_message> \"\"Authentication failed to \"\" </log_message>  <similarity> 0.58 </similarity><log_message> \"\"Key(%s), Bkey(%s) Unknown response : %s\"\" </log_message>  <similarity> 0.26 </similarity><log_message> \"\"Key(%s), Bkey(%s) Unknown response : %s\"\" </log_message>  <similarity> 0.26 </similarity><log_message> \"\"Unhandled state: \"\" </log_message>  <similarity> 0.26 </similarity><log_message> \"\"Mutation returned %s\"\" </log_message>  <similarity> 0.25 </similarity><log_message> \"\"Unhandled state: \"\" </log_message>  <similarity> 0.23 </similarity><log_message> \"\"Insertion to the collection failed : %s (type=%s, key=%s, attribut', 'output': b'\"protected String[] listSupportedSASLMechanisms(AtomicBoolean done) { final CountDownLatch listMechsLatch = new CountDownLatch(1); final AtomicReference<String> supportedMechs = new AtomicReference<String>(); Operation listMechsOp = opFact.saslMechs(new OperationCallback() { @Override public void receivedStatus(OperationStatus status) { if(status.isSuccess()) { supportedMechs.set(status.getMessage()); getLogger().debug(\"\"Received SASL supported mechs: \"\" + status.getMessage()); } else { getLogger().warn(\"\"Received non-success response for SASL mechs: \"\" + status); } } @Override public void complete() { listMechsLatch.countDown(); } }); conn.insertOperation(node, listMechsOp); try { if (!conn.isShutDown()) { listMechsLatch.await(); } else { done.set(true); // Connection is shutting down, tear.down. } } catch(InterruptedException ex) { getLogger().warn(\"\"Interrupted in Auth while waiting for SASL mechs.\"\"); // we can be interrupted if we were in the // process of auth\\'ing and the connection is // lost or dropped due to bad auth Thread.currentThread().interrupt(); if (listMechsOp != null) { listMechsOp.cancel(); } done.set(true); // If we were interrupted, tear down. } String supported = supportedMechs.get(); if (supported == null || supported.isEmpty()) { return null; } return supported.split(MECH_SEPARATOR); }\"'}\n",
            "{'input': b'\"@Override public void run(Matcher m) { <LOG> setServerStarted(); setServerStopped(); service.unregisterAction(getStartOK()); service.unregisterAction(getStartError()); service.unregisterAction(getStopped()); fireAfterStop(gracefulStop); }<log_message> \"\"Server started with errors\"\" </log_message>  <similarity> 0.69 </similarity><log_message> \"\"Server started with errors\"\" </log_message>  <similarity> 0.69 </similarity><log_message> \"\"SHUTDOWN_MSG: \"\" \"\"Shutting down \"\" \"\" at \"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Starting threads to send messages!\"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Starting threads to send messages!\"\" </log_message>  <similarity> 0.27 </similarity>\"', 'output': b'\"@Override public void run(Matcher m) { log.error(\"\"Server stopped before it started!\"\"); setServerStarted(); setServerStopped(); service.unregisterAction(getStartOK()); service.unregisterAction(getStartError()); service.unregisterAction(getStopped()); fireAfterStop(gracefulStop); }\"'}\n",
            "{'input': b'\"@Bean public HttpFirewall httpFirewall(final HawkbitSecurityProperties hawkbitSecurityProperties) { final List<String> allowedHostNames = hawkbitSecurityProperties.getAllowedHostNames(); final IgnorePathsStrictHttpFirewall firewall = new IgnorePathsStrictHttpFirewall( hawkbitSecurityProperties.getHttpFirewallIgnoredPaths()); if (!CollectionUtils.isEmpty(allowedHostNames)) { firewall.setAllowedHostnames(hostName -> { <LOG> return allowedHostNames.contains(hostName); }); } return firewall; }<log_message> \"\"Firewall check host: {}, allowed: {}\"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Types retained: {}\"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Could not find key in configurations: {}\"\" </log_message>  <similarity> 0.25 </similarity><log_message> \"\"There are not config ldifs given.\"\" </log_message>  <similarity> 0.25 </similarity><log_message> \"\"There are not schema ldifs given.\"\" </log_message>  <similarity> 0.25 </similarity>\"', 'output': b'\"@Bean public HttpFirewall httpFirewall(final HawkbitSecurityProperties hawkbitSecurityProperties) { final List<String> allowedHostNames = hawkbitSecurityProperties.getAllowedHostNames(); final IgnorePathsStrictHttpFirewall firewall = new IgnorePathsStrictHttpFirewall( hawkbitSecurityProperties.getHttpFirewallIgnoredPaths()); if (!CollectionUtils.isEmpty(allowedHostNames)) { firewall.setAllowedHostnames(hostName -> { log.debug(\"\"Firewall check host: {}, allowed: {}\"\", hostName, allowedHostNames.contains(hostName)); return allowedHostNames.contains(hostName); }); } return firewall; }\"'}\n",
            "{'input': b'\"public String getProperty(String key, boolean allowMissing) { String value = values.get(key); if (value == null && !allowMissing) { // Loudly alert in the log and throw an exception: String message = \"\"The property \\\\\"\"\"\" + key + \"\"\\\\\"\" doesn\\'t have a value.\"\"; <LOG> throw new IllegalArgumentException(message); // Or maybe kill ourselves, as a missing configuration parameter is // a serious error: // System.exit(1) } return value; }<log_message> </log_message>  <similarity> 0.32 </similarity><log_message> </log_message>  <similarity> 0.28 </similarity><log_message> </log_message>  <similarity> 0.28 </similarity><log_message> \"\"INVALID FORM: The form can only contain a-z, A-Z, 0-9, \\'-\\', \\'_\\', \\'()\\', \\'/\\', \\'+\\'. Given form: {}\"\" </log_message>  <similarity> 0.26 </similarity><log_message> </log_message>  <similarity> 0.25 </similarity>\"', 'output': b'\"public String getProperty(String key, boolean allowMissing) { String value = values.get(key); if (value == null && !allowMissing) { // Loudly alert in the log and throw an exception: String message = \"\"The property \\\\\"\"\"\" + key + \"\"\\\\\"\" doesn\\'t have a value.\"\"; log.error(message); throw new IllegalArgumentException(message); // Or maybe kill ourselves, as a missing configuration parameter is // a serious error: // System.exit(1) } return value; }\"'}\n"
          ]
        }
      ],
      "source": [
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(load_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqpAynnawoHx"
      },
      "outputs": [],
      "source": [
        "from tensorflow_datasets.core.utils.type_utils import Shape\n",
        "def preprocessing(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    x_input = tf.strings.strip(prefix + ex['input'])\n",
        "    y_label = tf.strings.strip(ex['output'])\n",
        "    inputs = tf.strings.join([x_input], separator=' ')\n",
        "    class_label = tf.strings.join([y_label], separator=' ')\n",
        "    return {'inputs': inputs, 'targets': class_label}\n",
        "  return ds.map(to_inputs_and_targets,\n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7VlXkDRml02",
        "outputId": "cca8e44e-6a97-4678-8754-7bcb69067b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A few preprocessed train examples...\n",
            "{'inputs': b'MULTI_LOG_INJECTION: augmented_input', 'targets': b'original_method'}\n",
            "{'inputs': b'MULTI_LOG_INJECTION: \"protected String[] listSupportedSASLMechanisms(AtomicBoolean done) { final CountDownLatch listMechsLatch = new CountDownLatch(1); final AtomicReference<String> supportedMechs = new AtomicReference<String>(); Operation listMechsOp = opFact.saslMechs(new OperationCallback() { @Override public void receivedStatus(OperationStatus status) { if(status.isSuccess()) { supportedMechs.set(status.getMessage()); getLogger().debug(\"\"Received SASL supported mechs: \"\" + status.getMessage()); } else { getLogger().warn(\"\"Received non-success response for SASL mechs: \"\" + status); } } @Override public void complete() { listMechsLatch.countDown(); } }); conn.insertOperation(node, listMechsOp); try { if (!conn.isShutDown()) { listMechsLatch.await(); } else { done.set(true); // Connection is shutting down, tear.down. } } catch(InterruptedException ex) { getLogger().warn(\"\"Interrupted in Auth while waiting for SASL mechs.\"\"); // we can be interrupted if we were in the // process of auth\\'ing and the connection is // lost or dropped due to bad auth Thread.currentThread().interrupt(); if (listMechsOp != null) { listMechsOp.cancel(); } done.set(true); // If we were interrupted, tear down. } String supported = supportedMechs.get(); if (supported == null || supported.isEmpty()) { return null; } return supported.split(MECH_SEPARATOR); }<log_message> \"\"Authenticated to \"\" </log_message>  <similarity> 0.58 </similarity><log_message> \"\"Authentication failed to \"\" </log_message>  <similarity> 0.58 </similarity><log_message> \"\"Key(%s), Bkey(%s) Unknown response : %s\"\" </log_message>  <similarity> 0.26 </similarity><log_message> \"\"Key(%s), Bkey(%s) Unknown response : %s\"\" </log_message>  <similarity> 0.26 </similarity><log_message> \"\"Unhandled state: \"\" </log_message>  <similarity> 0.26 </similarity><log_message> \"\"Mutation returned %s\"\" </log_message>  <similarity> 0.25 </similarity><log_message> \"\"Unhandled state: \"\" </log_message>  <similarity> 0.23 </similarity><log_message> \"\"Insertion to the collection failed : %s (type=%s, key=%s, attribut', 'targets': b'\"protected String[] listSupportedSASLMechanisms(AtomicBoolean done) { final CountDownLatch listMechsLatch = new CountDownLatch(1); final AtomicReference<String> supportedMechs = new AtomicReference<String>(); Operation listMechsOp = opFact.saslMechs(new OperationCallback() { @Override public void receivedStatus(OperationStatus status) { if(status.isSuccess()) { supportedMechs.set(status.getMessage()); getLogger().debug(\"\"Received SASL supported mechs: \"\" + status.getMessage()); } else { getLogger().warn(\"\"Received non-success response for SASL mechs: \"\" + status); } } @Override public void complete() { listMechsLatch.countDown(); } }); conn.insertOperation(node, listMechsOp); try { if (!conn.isShutDown()) { listMechsLatch.await(); } else { done.set(true); // Connection is shutting down, tear.down. } } catch(InterruptedException ex) { getLogger().warn(\"\"Interrupted in Auth while waiting for SASL mechs.\"\"); // we can be interrupted if we were in the // process of auth\\'ing and the connection is // lost or dropped due to bad auth Thread.currentThread().interrupt(); if (listMechsOp != null) { listMechsOp.cancel(); } done.set(true); // If we were interrupted, tear down. } String supported = supportedMechs.get(); if (supported == null || supported.isEmpty()) { return null; } return supported.split(MECH_SEPARATOR); }\"'}\n",
            "{'inputs': b'MULTI_LOG_INJECTION: \"@Override public void run(Matcher m) { <LOG> setServerStarted(); setServerStopped(); service.unregisterAction(getStartOK()); service.unregisterAction(getStartError()); service.unregisterAction(getStopped()); fireAfterStop(gracefulStop); }<log_message> \"\"Server started with errors\"\" </log_message>  <similarity> 0.69 </similarity><log_message> \"\"Server started with errors\"\" </log_message>  <similarity> 0.69 </similarity><log_message> \"\"SHUTDOWN_MSG: \"\" \"\"Shutting down \"\" \"\" at \"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Starting threads to send messages!\"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Starting threads to send messages!\"\" </log_message>  <similarity> 0.27 </similarity>\"', 'targets': b'\"@Override public void run(Matcher m) { log.error(\"\"Server stopped before it started!\"\"); setServerStarted(); setServerStopped(); service.unregisterAction(getStartOK()); service.unregisterAction(getStartError()); service.unregisterAction(getStopped()); fireAfterStop(gracefulStop); }\"'}\n",
            "{'inputs': b'MULTI_LOG_INJECTION: \"@Bean public HttpFirewall httpFirewall(final HawkbitSecurityProperties hawkbitSecurityProperties) { final List<String> allowedHostNames = hawkbitSecurityProperties.getAllowedHostNames(); final IgnorePathsStrictHttpFirewall firewall = new IgnorePathsStrictHttpFirewall( hawkbitSecurityProperties.getHttpFirewallIgnoredPaths()); if (!CollectionUtils.isEmpty(allowedHostNames)) { firewall.setAllowedHostnames(hostName -> { <LOG> return allowedHostNames.contains(hostName); }); } return firewall; }<log_message> \"\"Firewall check host: {}, allowed: {}\"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Types retained: {}\"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Could not find key in configurations: {}\"\" </log_message>  <similarity> 0.25 </similarity><log_message> \"\"There are not config ldifs given.\"\" </log_message>  <similarity> 0.25 </similarity><log_message> \"\"There are not schema ldifs given.\"\" </log_message>  <similarity> 0.25 </similarity>\"', 'targets': b'\"@Bean public HttpFirewall httpFirewall(final HawkbitSecurityProperties hawkbitSecurityProperties) { final List<String> allowedHostNames = hawkbitSecurityProperties.getAllowedHostNames(); final IgnorePathsStrictHttpFirewall firewall = new IgnorePathsStrictHttpFirewall( hawkbitSecurityProperties.getHttpFirewallIgnoredPaths()); if (!CollectionUtils.isEmpty(allowedHostNames)) { firewall.setAllowedHostnames(hostName -> { log.debug(\"\"Firewall check host: {}, allowed: {}\"\", hostName, allowedHostNames.contains(hostName)); return allowedHostNames.contains(hostName); }); } return firewall; }\"'}\n",
            "{'inputs': b'MULTI_LOG_INJECTION: \"public String getProperty(String key, boolean allowMissing) { String value = values.get(key); if (value == null && !allowMissing) { // Loudly alert in the log and throw an exception: String message = \"\"The property \\\\\"\"\"\" + key + \"\"\\\\\"\" doesn\\'t have a value.\"\"; <LOG> throw new IllegalArgumentException(message); // Or maybe kill ourselves, as a missing configuration parameter is // a serious error: // System.exit(1) } return value; }<log_message> </log_message>  <similarity> 0.32 </similarity><log_message> </log_message>  <similarity> 0.28 </similarity><log_message> </log_message>  <similarity> 0.28 </similarity><log_message> \"\"INVALID FORM: The form can only contain a-z, A-Z, 0-9, \\'-\\', \\'_\\', \\'()\\', \\'/\\', \\'+\\'. Given form: {}\"\" </log_message>  <similarity> 0.26 </similarity><log_message> </log_message>  <similarity> 0.25 </similarity>\"', 'targets': b'\"public String getProperty(String key, boolean allowMissing) { String value = values.get(key); if (value == null && !allowMissing) { // Loudly alert in the log and throw an exception: String message = \"\"The property \\\\\"\"\"\" + key + \"\"\\\\\"\" doesn\\'t have a value.\"\"; log.error(message); throw new IllegalArgumentException(message); // Or maybe kill ourselves, as a missing configuration parameter is // a serious error: // System.exit(1) } return value; }\"'}\n"
          ]
        }
      ],
      "source": [
        "print(\"A few preprocessed train examples...\")\n",
        "sample = tfds.as_numpy(preprocessing(load_dataset(\"train\").take(5)))\n",
        "for ex in sample:\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYRWL9r-wpou",
        "outputId": "9cb5aba3-b557-4de6-90bc-bf5be41d203e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<seqio.dataset_providers.Mixture at 0x7cf484767820>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True, required=False),\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True)\n",
        "    }\n",
        "\n",
        "# TASK\n",
        "t5.data.TaskRegistry.remove(TASK_NAME)\n",
        "t5.data.TaskRegistry.add(\n",
        "    TASK_NAME,\n",
        "    dataset_fn=load_dataset,\n",
        "    splits=[\"train\",\n",
        "            \"validation\",\n",
        "            \"test\"\n",
        "            ],\n",
        "    text_preprocessor=[preprocessing],\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES\n",
        ")\n",
        "\n",
        "MIXTURE_NAME = \"finetuning\" #@param{ type : \"string\"}\n",
        "\n",
        "# MIXTURE\n",
        "t5.data.MixtureRegistry.remove(MIXTURE_NAME)\n",
        "t5.data.MixtureRegistry.add(\n",
        "    MIXTURE_NAME,\n",
        "    [TASK_NAME],\n",
        "    default_rate=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtl_lVPOmpaC",
        "outputId": "fe1dd373-0dc2-46db-f0e2-83e0d6c46edb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'MULTI_LOG_INJECTION: \"@Override public boolean schemaExists(ConnectorSession session, String schemaName) { BigQueryClient client = bigQueryClientFactory.create(session); DatasetId localDatasetId = client.toDatasetId(schemaName); // Overridden to make sure an error message is returned in case of an ambiguous schema name <LOG> return client.toRemoteDataset(localDatasetId) .map(RemoteDatabaseObject::getOnlyRemoteName) .filter(remoteSchema -> client.getDataset(DatasetId.of(localDatasetId.getProject(), remoteSchema)) != null) .isPresent(); }<log_message> \"\"listTables(session=%s, schemaName=%s)\"\" </log_message>  <similarity> 0.35 </similarity><log_message> \"\"listTables(session=%s, schemaName=%s)\"\" </log_message>  <similarity> 0.35 </similarity><log_message> \"\"Filtered out [%s] from list of tables due to ambiguous name\"\" </log_message>  <similarity> 0.28 </similarity><log_message> \"\"Filtered out [%s] from list of tables due to ambiguous name\"\" </log_message>  <similarity> 0.28 </similarity><log_message> \"\"getTableHandle(session=%s, schemaTableName=%s)\"\" </log_message>  <similarity> 0.25 </similarity><log_message> \"\"Table [%s.%s] was not found\"\" </log_message>  <similarity> 0.25 </similarity>\"', 'inputs': array([   41, 23774,    61, 13620,    61,  2290,   706,  4337, 10083,\n",
            "          73, 19037, 11594,    35,   188,  3709,  7545,   572,  8187,\n",
            "        1577,   938,     3,    67, 25003,    81,    24,  2540,   975,\n",
            "        1065,   660,    43,   578,   975,  1065,   676,     5,  9051,\n",
            "         572, 14475,  5005, 17267,   190,   368, 10296,   190,    43,\n",
            "         660,     5,   409, 10296,   190,   572, 15323,   219,  5005,\n",
            "       19009,  2451, 21697,    10,   127,   352,    65,  1434,   618,\n",
            "          20,  3979,    16,   351,    12,    65,   345, 11657, 15011,\n",
            "        3709,   209,    77, 13620,  1678,    40,   660,     5,   409,\n",
            "        8764, 10296,   572, 11675, 10296,   190,    81,    14,  5645,\n",
            "         572,  8764,  5411,   589, 12431,  2406,  4945,  8764,   219,\n",
            "          81,    14, 14313,   572, 30955,  2212,   781,   660,     5,\n",
            "        2406, 10296,   572, 10296,   190,     5,  1257,   572, 11675,\n",
            "       10296,   190,     5,  2406,  2803,  4566,     3,  2628,  2212,\n",
            "          81,    81,   275,    99,    81,    14,   784,  9590,  4566,\n",
            "         151,    25,  7682,  3385,    61, 12697,  1678,  1674,  2770,\n",
            "        1078,     7,   572, 14475, 16363,     7,     3, 25003, 16363,\n",
            "           7,  4574,    57,    77,   100,  3385,    61, 12697,  1678,\n",
            "          77, 29244,   728,  1678, 22320,   550,    77,   100, 29244,\n",
            "         728, 19838,  3385,    61, 12697,  1678,  1674,  2770,  1078,\n",
            "           7,   572, 14475, 16363,     7,     3, 25003, 16363,     7,\n",
            "        4574,    57,    77,   100,  3385,    61, 12697,  1678,    77,\n",
            "       29244,   728,  1678, 22320,   550,    77,   100, 29244,   728,\n",
            "       19838,  3385,    61, 12697,  1678,  1674,  1300,    74,    86,\n",
            "         163,  1315,     7,   935,    50,   360,    12,  4182,   791,\n",
            "          10,   345, 11657, 15011,   209,    57,    57,    77,   100,\n",
            "        3385,    61, 12697,  1678,    77, 29244,   728,  1678, 17900,\n",
            "         274,    77,   100, 29244,   728, 19838,  3385,    61, 12697,\n",
            "        1678,  1674,  1300,    74,    86,   163,  1315,     7,   935,\n",
            "          50,   360,    12,  4182,   791,    10,   345, 11657, 15011,\n",
            "         209,    57,    57,    77,   100,  3385,    61, 12697,  1678,\n",
            "          77, 29244,   728,  1678, 17900,   274,    77,   100, 29244,\n",
            "         728, 19838,  3385,    61, 12697,  1678,  1674,  2406,  1078,\n",
            "        3328,   572, 14475, 16363,     7,     3,  3709, 10333, 16363,\n",
            "           7,  4574,    57,    77,   100,  3385,    61, 12697,  1678,\n",
            "          77, 29244,   728,  1678, 17900,   550,    77,   100, 29244,\n",
            "         728, 19838,  3385,    61, 12697,  1678,  1674,  1078,   163,\n",
            "        1315,     7,     5,  1315,     7,   935,    62,    72,   398,\n",
            "          57,    57,    77,   100,  3385,    61, 12697,  1678,    77,\n",
            "       29244,   728,  1678, 17900,   550,    77,   100, 29244,   728,\n",
            "        6628,     1], dtype=int32), 'targets_pretokenized': b'\"@Override public boolean schemaExists(ConnectorSession session, String schemaName) { BigQueryClient client = bigQueryClientFactory.create(session); DatasetId localDatasetId = client.toDatasetId(schemaName); // Overridden to make sure an error message is returned in case of an ambiguous schema name log.debug(\"\"schemaExists(session=%s)\"\", session); return client.toRemoteDataset(localDatasetId) .map(RemoteDatabaseObject::getOnlyRemoteName) .filter(remoteSchema -> client.getDataset(DatasetId.of(localDatasetId.getProject(), remoteSchema)) != null) .isPresent(); }\"', 'targets': array([19037, 11594,    35,   188,  3709,  7545,   572,  8187,  1577,\n",
            "         938,     3,    67, 25003,    81,    24,  2540,   975,  1065,\n",
            "         660,    43,   578,   975,  1065,   676,     5,  9051,   572,\n",
            "       14475,  5005, 17267,   190,   368, 10296,   190,    43,   660,\n",
            "           5,   409, 10296,   190,   572, 15323,   219,  5005, 19009,\n",
            "        2451, 21697,    10,   127,   352,    65,  1434,   618,    20,\n",
            "        3979,    16,   351,    12,    65,   345, 11657, 15011,  3709,\n",
            "         209,  1483,     5, 31072, 13918,    57, 15323,  7545,   572,\n",
            "       14475, 16363,     7,  4574,    57,     3,   938,  5005,    40,\n",
            "         660,     5,   409,  8764, 10296,   572, 11675, 10296,   190,\n",
            "          81,    14,  5645,   572,  8764,  5411,   589, 12431,  2406,\n",
            "        4945,  8764,   219,    81,    14, 14313,   572, 30955,  2212,\n",
            "         781,   660,     5,  2406, 10296,   572, 10296,   190,     5,\n",
            "        1257,   572, 11675, 10296,   190,     5,  2406,  2803,  4566,\n",
            "           3,  2628,  2212,    81,    81,   275,    99,    81,    14,\n",
            "         784,  9590,  4566,   151,    25,    57,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'MULTI_LOG_INJECTION: \"@Override public boolean isSatisfied() throws Exception { <LOG> return bytesRead.get() == FRAME_SIZE || !transport.isConnected(); }<log_message> \"\"Checking completion: read {} expecting {}\"\" </log_message>  <similarity> 0.77 </similarity><log_message> \"\"Are we complete: error:{} nessages read:{}\"\" </log_message>  <similarity> 0.50 </similarity><log_message> \"\"Checking for : X sent, System Memory Usage \"\" \"\", sent: \"\" </log_message>  <similarity> 0.47 </similarity><log_message> \"\"received count:\"\" </log_message>  <similarity> 0.47 </similarity><log_message> \"\"received count:\"\" </log_message>  <similarity> 0.47 </similarity>\"', 'inputs': array([   41, 23774,    61, 13620,    61,  2290,   706,  4337, 10083,\n",
            "          73, 19037, 11594,    35,   188,    20, 25971,  4566,   143,\n",
            "         341,    24,    77, 13620,  1678,    40,  3475,  2435,     5,\n",
            "        2406,  4566,   212,    41, 30840,    61,  9100,  1015,   510,\n",
            "       10292,  2071,     5,   784, 14311,  4566,   151,    25,  7682,\n",
            "        3385,    61, 12697,  1678,  1674,  2905,    64,  5001,    73,\n",
            "         397,    24,  8843,  9862,    24,  6800,    57,    77,   100,\n",
            "        3385,    61, 12697,  1678,    77, 29244,   728,  1678, 23187,\n",
            "         585,    77,   100, 29244,   728, 19838,  3385,    61, 12697,\n",
            "        1678,  1674, 12307,    68,   597,    73,  1434,    73, 28984,\n",
            "          41,   741,  9068,   397,    73, 28984,    57,    57,    77,\n",
            "         100,  3385,    61, 12697,  1678,    77, 29244,   728,  1678,\n",
            "       11204,   392,    77,   100, 29244,   728, 19838,  3385,    61,\n",
            "       12697,  1678,  1674,  2905,    64,    23,   226,   712,  1769,\n",
            "           3,   905,  8353, 21305,  1674,  1674,     3,  1769,    73,\n",
            "        1674,    77,   100,  3385,    61, 12697,  1678,    77, 29244,\n",
            "         728,  1678, 23607,   585,    77,   100, 29244,   728, 19838,\n",
            "        3385,    61, 12697,  1678,  1674,   116,  1317,   977,    45,\n",
            "        1237,  9317,    57,    77,   100,  3385,    61, 12697,  1678,\n",
            "          77, 29244,   728,  1678, 23607,   585,    77,   100, 29244,\n",
            "         728, 19838,  3385,    61, 12697,  1678,  1674,   116,  1317,\n",
            "         977,    45,  1237,  9317,    57,    77,   100,  3385,    61,\n",
            "       12697,  1678,    77, 29244,   728,  1678, 23607,   585,    77,\n",
            "         100, 29244,   728,  6628,     1], dtype=int32), 'targets_pretokenized': b'\"@Override public boolean isSatisfied() throws Exception { LOG.debug(\"\"Checking completion: read {} expecting {}\"\", bytesRead.get(), FRAME_SIZE); return bytesRead.get() == FRAME_SIZE || !transport.isConnected(); }\"', 'targets': array([19037, 11594,    35,   188,    20, 25971,  4566,   143,   341,\n",
            "          24, 17223,     5, 31072, 13918,    57,  2905,    64,  5001,\n",
            "          73,   397,    24,  8843,  9862,    24,  6800,    57,     3,\n",
            "        3475,  2435,     5,  2406,  4566,     3,    41, 30840,    61,\n",
            "        9100,  5005,    40,  3475,  2435,     5,  2406,  4566,   212,\n",
            "          41, 30840,    61,  9100,  1015,   510, 10292,  2071,     5,\n",
            "         784, 14311,  4566,   151,    25,    57,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'MULTI_LOG_INJECTION: \"@Override protected void parse(String recordEntry, OutputCollector<ChukwaRecordKey, ChukwaRecord> output, Reporter reporter) throws Throwable { Logger log = Logger.getLogger(HBaseMasterProcessor.class); long timeStamp = Calendar.getInstance(TimeZone.getTimeZone(\"\"UTC\"\")) .getTimeInMillis(); ChukwaRecord record = new ChukwaRecord(); Map<String, Buffer> metricsMap = new HashMap<String, Buffer>(); try { JSONObject obj = (JSONObject) JSONValue.parse(recordEntry); String ttTag = chunk.getTag(\"\"timeStamp\"\"); if (ttTag == null) { <LOG> } else { timeStamp = Long.parseLong(ttTag); } @SuppressWarnings(\"\"unchecked\"\") Iterator<Map.Entry<String, ?>> keys = obj.entrySet().iterator(); while (keys.hasNext()) { Map.Entry<String, ?> entry = keys.next(); String key = entry.getKey(); Object value = entry.getValue(); String valueString = value == null ? \"\"\"\" : value.toString(); // Calculate rate for some of the metrics if (rateMap.containsKey(key)) { long oldValue = rateMap.get(key); long curValue = Long.parseLong(valueString); rateMap.put(key, curValue); long newValue = curValue - oldValue; if (newValue < 0) { <LOG> newValue = 0L; } valueString = Long.toString(newValue); } Buffer b = new Buffer(valueString.getBytes(Charset.forName(\"\"UTF-8\"\"))); metricsMap.put(key, b); } TreeMap<String, Buffer> t = new TreeMap<String, Buffer>(metricsMap); record.setMapFields(t); buildGenericRecord(record, null, timeStamp, \"\"master\"\"); output.collect(key, record); } catch (Exception e) { <LOG> } }<log_message> \"\"timeStamp tag not set in JMX adaptor for hbase region server\"\" </log_message>  <similarity> 0.80 </similarity><log_message> </log_message>  <similarity> 0.80 </similarity><log_message> \"\"timeStamp tag not set in JMX adaptor for zookeeper\"\" </log_message>  <similarity> 0.62 </similarity><log_message> </log_message>  <similarity> 0.62 </similarity><log_message> \"\"Unparsable data:\"\" </log_message>  <similarity> 0.29 </similarity><log_message> </log_message>  <similarity> 0.29 </similarity><log_message> </log_message>  <similarity> 0.29 </similar', 'inputs': array([   41, 23774,    61, 13620,    61,  2290,   706,  4337, 10083,\n",
            "          73, 19037, 11594,   309,    71,  1131,   572,   680,   954,\n",
            "        1272,     3, 12321, 10654,  7682, 10506,  2474,  3728,  1802,\n",
            "         506,     3, 13790,   401,  3728,  1802,  1678,  1463,     3,\n",
            "        3984,   105, 12413,    81,   143,  2800,    24, 14521,  1483,\n",
            "          43, 14521,     5,  2406,  7811,   572,   664,  2282,  6993,\n",
            "        4423,     5,  2911,  5005,   198,    93, 24768,    43,  5201,\n",
            "           5,  2406,  1332,   572, 16263,     5,  2406, 16263, 13918,\n",
            "          57, 24604,    57,    57,    81,    81,    14,  2406,   876,\n",
            "       27591,  4566,   151, 13790,   401,  3728,  1802,   954,    43,\n",
            "          56, 13790,   401,  3728,  1802,  4566,   151,   560,  7682,\n",
            "         680,     3, 12007,  1678,  8113,   562,    43,    56,  3764,\n",
            "        7682,   680,     3, 12007,  1678,  4566,   151,   293,    24,\n",
            "       12029,  1145,    43,     6,  6349,   589,    81,  9298,   321,\n",
            "           5, 21736,   572, 17298,  1272,  5005,    67,   451,    55,\n",
            "        2703,    43, 10669,     5,  2406,  2703, 13918,    57,   881,\n",
            "       24768,    57,    57,  5005,    66,     6,    55,    55,  2703,\n",
            "         212,    99,    81,    24,    77, 13620,  1678,    25,   371,\n",
            "          24,    93, 24768,    43,   903,     5, 21736,  2560,   572,\n",
            "          55,    55,  2703,  5005,    25,    60, 26495, 11033,     7,\n",
            "       13918,    57,  4709,    57,    57,    81,  4957,  7682,   562,\n",
            "           5,  1272,  7682,   680,     3,   306, 17245,  3882,    43,\n",
            "        1145,     5, 14071,   691,  4566,     5,   882, 10635,  4566,\n",
            "         151,   240,     6,  2445,     7,     5,  9348,  4724,  4566,\n",
            "          81,    24,   560,     5,  1272,  7682,   680,     3,   306,\n",
            "        1678,   748,    43,  3882,     5, 18582,  4566,   151,    67,\n",
            "         289,    43,   748,     5,  2406,   506,  4566,   151,   259,\n",
            "         165,    43,   748,     5,  2406,   321,  4566,   151,    67,\n",
            "         165,   680,    43,   165,   212,    99,   306,  1674,    57,\n",
            "          57,   226,   165,     5,   409,   680,  4566,   151, 19009,\n",
            "          41, 22256,  1085,    23,   126,    12,     4,  8113,    66,\n",
            "           6,  3697,   562,     5, 31776,   506,   572,  2445,    81,\n",
            "          81,    24,   198, 14838,    43,  1085,   562,     5,  2406,\n",
            "         572,  2445,  5005,   198,  4450,   321,    43,   903,     5,\n",
            "       21736,  2560,   572,  3356,   680,  5005,  1085,   562,     5,\n",
            "        6805,   572,  2445,     3,  4450,   321,  5005,   198,  8216,\n",
            "          43,  4450,   321,   117, 14838,   151,    66,     6,  3687,\n",
            "         321,    77,   170,    81,    24,    77, 13620,  1678,  8216,\n",
            "          43,   170,   462,   151,    25,   165,   680,    43,   903,\n",
            "           5,   409,   680,   572,  3687,   321,  5005,    25, 12007,\n",
            "         489,    43,    56, 12007,   572,  3356,   680,     5,  2406,\n",
            "        2508,   572, 20785,     5,  1730,   219, 13918,    57, 12467,\n",
            "       20968,    57,    81,    81,  5005,  8113,   562,     5,  6805,\n",
            "         572,  2445,     3,   489,  5005,    25,  3322,   562,  7682,\n",
            "         680,     3, 12007,  1678,   451,    43,    56,  3322,   562,\n",
            "        7682,   680,     3, 12007,  1678,   572,  9181,     7,   562,\n",
            "        5005,   954,     5,  2076,   562,  3230,   572,    55,  5005,\n",
            "         420, 10702,  1802,   572, 17298,     3,    99,     3,    93,\n",
            "       24768,     3,  1674,  6809,    57,    57,  5005,  1463,     5,\n",
            "       27051,   572,  2445,     3,   954,  5005,    25,   569,     6,\n",
            "         161,   191,    81,    24,    77, 13620,  1678,    25,    25,\n",
            "        7682,  3385,    61, 12697,  1678,  1674,   881, 24768,  2429,\n",
            "          72,    97,    16,   444, 15309,  5432,   285,    23,  1685,\n",
            "        5811,  1392,  1274,    57,    57,    77,   100,  3385,    61,\n",
            "       12697,  1678,    77, 29244,   728,  1678, 25149,   392,    77,\n",
            "         100, 29244,   728, 19838,  3385,    61, 12697,  1678,    77,\n",
            "         100,  3385,    61, 12697,  1678,    77, 29244,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'\"@Override protected void parse(String recordEntry, OutputCollector<ChukwaRecordKey, ChukwaRecord> output, Reporter reporter) throws Throwable { Logger log = Logger.getLogger(HBaseMasterProcessor.class); long timeStamp = Calendar.getInstance(TimeZone.getTimeZone(\"\"UTC\"\")) .getTimeInMillis(); ChukwaRecord record = new ChukwaRecord(); Map<String, Buffer> metricsMap = new HashMap<String, Buffer>(); try { JSONObject obj = (JSONObject) JSONValue.parse(recordEntry); String ttTag = chunk.getTag(\"\"timeStamp\"\"); if (ttTag == null) { log.warn(\"\"timeStamp tag not set in JMX adaptor for hbase master\"\"); } else { timeStamp = Long.parseLong(ttTag); } @SuppressWarnings(\"\"unchecked\"\") Iterator<Map.Entry<String, ?>> keys = obj.entrySet().iterator(); while (keys.hasNext()) { Map.Entry<String, ?> entry = keys.next(); String key = entry.getKey(); Object value = entry.getValue(); String valueString = value == null ? \"\"\"\" : value.toString(); // Calculate rate for some of the metrics if (rateMap.containsKey(key)) { long oldValue = rateMap.get(key); long curValue = Long.parseLong(valueString); rateMap.put(key, curValue); long newValue = curValue - oldValue; if (newValue < 0) { log.warn(\"\"HBaseMaster rateMap might be reset or corrupted for metric \"\" + key); newValue = 0L; } valueString = Long.toString(newValue); } Buffer b = new Buffer(valueString.getBytes(Charset.forName(\"\"UTF-8\"\"))); metricsMap.put(key, b); } TreeMap<String, Buffer> t = new TreeMap<String, Buffer>(metricsMap); record.setMapFields(t); buildGenericRecord(record, null, timeStamp, \"\"master\"\"); output.collect(key, record); } catch (Exception e) { log.error(ExceptionUtil.getStackTrace(e)); } }\"', 'targets': array([19037, 11594,   309,    71,  1131,   572,   680,   954,  1272,\n",
            "           3, 12321, 10654,  7682, 10506,  2474,  3728,  1802,   506,\n",
            "           3, 13790,   401,  3728,  1802,  1678,  1463,     3,  3984,\n",
            "         105, 12413,    81,   143,  2800,    24, 14521,  1483,    43,\n",
            "       14521,     5,  2406,  7811,   572,   664,  2282,  6993,  4423,\n",
            "           5,  2911,  5005,   198,    93, 24768,    43,  5201,     5,\n",
            "        2406,  1332,   572, 16263,     5,  2406, 16263, 13918,    57,\n",
            "       24604,    57,    57,    81,    81,    14,  2406,   876, 27591,\n",
            "        4566,   151, 13790,   401,  3728,  1802,   954,    43,    56,\n",
            "       13790,   401,  3728,  1802,  4566,   151,   560,  7682,   680,\n",
            "           3, 12007,  1678,  8113,   562,    43,    56,  3764,  7682,\n",
            "         680,     3, 12007,  1678,  4566,   151,   293,    24, 12029,\n",
            "        1145,    43,     6,  6349,   589,    81,  9298,   321,     5,\n",
            "       21736,   572, 17298,  1272,  5005,    67,   451,    55,  2703,\n",
            "          43, 10669,     5,  2406,  2703, 13918,    57,   881, 24768,\n",
            "          57,    57,  5005,    66,     6,    55,    55,  2703,   212,\n",
            "          99,    81,    24,  1483,     5,  4505,    31, 13918,    57,\n",
            "         881, 24768,  2429,    72,    97,    16,   444, 15309,  5432,\n",
            "         285,    23,  1685,  5811,  1786,    57,    57,  5005,    25,\n",
            "         371,    24,    93, 24768,    43,   903,     5, 21736,  2560,\n",
            "         572,    55,    55,  2703,  5005,    25,    60, 26495, 11033,\n",
            "           7, 13918,    57,  4709,    57,    57,    81,  4957,  7682,\n",
            "         562,     5,  1272,  7682,   680,     3,   306, 17245,  3882,\n",
            "          43,  1145,     5, 14071,   691,  4566,     5,   882, 10635,\n",
            "        4566,   151,   240,     6,  2445,     7,     5,  9348,  4724,\n",
            "        4566,    81,    24,   560,     5,  1272,  7682,   680,     3,\n",
            "         306,  1678,   748,    43,  3882,     5, 18582,  4566,   151,\n",
            "          67,   289,    43,   748,     5,  2406,   506,  4566,   151,\n",
            "         259,   165,    43,   748,     5,  2406,   321,  4566,   151,\n",
            "          67,   165,   680,    43,   165,   212,    99,   306,  1674,\n",
            "          57,    57,   226,   165,     5,   409,   680,  4566,   151,\n",
            "       19009,    41, 22256,  1085,    23,   126,    12,     4,  8113,\n",
            "          66,     6,  3697,   562,     5, 31776,   506,   572,  2445,\n",
            "          81,    81,    24,   198, 14838,    43,  1085,   562,     5,\n",
            "        2406,   572,  2445,  5005,   198,  4450,   321,    43,   903,\n",
            "           5, 21736,  2560,   572,  3356,   680,  5005,  1085,   562,\n",
            "           5,  6805,   572,  2445,     3,  4450,   321,  5005,   198,\n",
            "        8216,    43,  4450,   321,   117, 14838,   151,    66,     6,\n",
            "        3687,   321,    77,   170,    81,    24,  1483,     5,  4505,\n",
            "          31, 13918,    57,   664,  2282,  6993,  1085,   562,   481,\n",
            "          42,  3224,    46, 16897,    74,    23,  8170,  1674,   135,\n",
            "         289,  5005,  8216,    43,   170,   462,   151,    25,   165,\n",
            "         680,    43,   903,     5,   409,   680,   572,  3687,   321,\n",
            "        5005,    25, 12007,   489,    43,    56, 12007,   572,  3356,\n",
            "         680,     5,  2406,  2508,   572, 20785,     5,  1730,   219,\n",
            "       13918,    57, 12467, 20968,    57,    81,    81,  5005,  8113,\n",
            "         562,     5,  6805,   572,  2445,     3,   489,  5005,    25,\n",
            "        3322,   562,  7682,   680,     3, 12007,  1678,   451,    43,\n",
            "          56,  3322,   562,  7682,   680,     3, 12007,  1678,   572,\n",
            "        9181,     7,   562,  5005,   954,     5,  2076,   562,  3230,\n",
            "         572,    55,  5005,   420, 10702,  1802,   572, 17298,     3,\n",
            "          99,     3,    93, 24768,     3,  1674,  6809,    57,    57,\n",
            "        5005,  1463,     5, 27051,   572,  2445,     3,   954,  5005,\n",
            "          25,   569,     6,   161,   191,    81,    24,  1483,     5,\n",
            "        8952,   572,   161,  1682,     5,  2406, 25139,   572,   123,\n",
            "          81,  5005,    25,    25,    57,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'MULTI_LOG_INJECTION: \"@Inject(at = @At(value = \"\"JUMP\"\", ordinal = 8), method = \"\"setupServer()Z\"\") // TODO keep ordinal updated private void init(CallbackInfoReturnable<Boolean> ci) { // Register Bukkit Enchantments for (Enchantment enchantment : Registry.ENCHANTMENT) org.bukkit.enchantments.Enchantment.registerEnchantment(new CardboardEnchantment(enchantment)); CraftMagicNumbers.test(); CraftMagicNumbers.setupUnknownModdedMaterials(); MinecraftDedicatedServer thiss = (MinecraftDedicatedServer) (Object) this; ((MinecraftDedicatedServer) (Object) this).setPlayerManager(new DedicatedPlayerManager(thiss, thiss.getRegistryManager(), saveHandler)); Bukkit.setServer(new CraftServer((MinecraftDedicatedServer) (Object) this)); org.spigotmc.SpigotConfig.init(new File(\"\"spigot.yml\"\")); Bukkit.getLogger().info(\"\"Loading Bukkit plugins...\"\"); File pluginsDir = new File(\"\"plugins\"\"); pluginsDir.mkdir(); Bukkit.getPluginManager().registerInterface(JavaPluginLoader.class); CraftServer s = ((CraftServer)Bukkit.getServer()); if (CraftServer.server == null) CraftServer.server = (MinecraftDedicatedServer) (Object) this; s.loadPlugins(); s.enablePlugins(PluginLoadOrder.STARTUP); Bukkit.getLogger().info(\"\"\"\"); }<log_message> \"\" issued server command: \"\" </log_message>  <similarity> 0.15 </similarity><log_message> \"\"tryHarvestBlock(): expected to pop \"\" \"\" but got \"\" </log_message>  <similarity> 0.13 </similarity><log_message> \"\"Remaining cause: \"\" </log_message>  <similarity> 0.13 </similarity><log_message> \"\"Failed to handle entity type \"\" </log_message>  <similarity> 0.12 </similarity><log_message> \"\"Failed to handle entity type \"\" </log_message>  <similarity> 0.12 </similarity><log_message> </log_message>  <similarity> 0.12 </similarity>\"', 'inputs': array([   41, 23774,    61, 13620,    61,  2290,   706,  4337, 10083,\n",
            "          73, 19037, 26316,   572,  1119,    43,    60,  3038,   572,\n",
            "        3356,    43,  1674,   706, 28723,    57,    57,     3, 19297,\n",
            "          43,   543,   284,   947,    43,  1674,  2076,   591,  1466,\n",
            "        4566,  1225,    57,    57,    81, 19009,  2910,  6731,   422,\n",
            "       19297,  2095,   186,    71,  2128,   572,  3599,   531,  4908,\n",
            "         308,  7682,  4380,  1678, 11477,    81,    24, 19009,  9386,\n",
            "          41, 31445,    41, 25771,  5238,    23,     6, 25771,   834,\n",
            "        1771,  9296,    55,   834,   226, 12846,     5,   454, 24048,\n",
            "        7285,  6914,    81,   793,     5,   243,  2474, 12536,     5,\n",
            "       19178,  1098,  5238,     5, 25771,   834,     5, 15797, 25771,\n",
            "         834,   572,  3687,  3630,  2896, 25771,   834,   572, 19178,\n",
            "        1098,   834,    81,  5005,  7625, 25185,  1700,     7,     5,\n",
            "        1382,  4566,   151,  7625, 25185,  1700,     7,     5,  2076,\n",
            "         591, 10076, 13631,    45,    74, 11624,     7,  4566,   151,\n",
            "       23884, 25173,  1466,    37,     7,    43,     6,   452,  1056,\n",
            "        8062, 25173,  1466,    81,     6,   589,    81,    37,   151,\n",
            "           6,   572,   452,  1056,  8062, 25173,  1466,    81,     6,\n",
            "         589,    81,    37,   322,  2076,  6885,   659,   572,  3687,\n",
            "          41, 25173,  6885,   659,   572,  7122,     7,     3,    37,\n",
            "           7,     5,  2406,  3252,   659,  4566,     3,   702,  1140,\n",
            "          81,  5005,    41, 31445,     5,  2076,  1466,   572,  3687,\n",
            "        7625,  1466, 26120,   452,  1056,  8062, 25173,  1466,    81,\n",
            "           6,   589,    81,    37,    81,  5005,   793,     5,     7,\n",
            "        5599, 11042,   101,    90,     5, 17970, 11042,   612,     5,\n",
            "       18231,   572,  3687,   856, 13918,    57,     7,  5599, 11042,\n",
            "           5,   125,  5104,    57,    57,    81,  5005,    41, 31445,\n",
            "           5,  2406,  7811,  4566,     5,  8104, 13918,    57, 20815,\n",
            "          41, 31445, 15994, 19031,    57,  5005,   856, 15994,  2361,\n",
            "          43,    56,   856, 13918,    57, 24656,     7,    57,    57,\n",
            "        5005, 15994,  2361,     5,   101,   401,  7399,  4566,   151,\n",
            "          41, 31445,     5,  2406,  4702,   659,  4566,     5, 15797,\n",
            "        3640,   572,  5621,  4702,  3601,     5,  2911,  5005,  7625,\n",
            "        1466,   389,    43,     6,   572, 24843,  1466,    81, 31445,\n",
            "           5,  2406,  1466,  4566,  5005,    66,     6, 24843,  1466,\n",
            "           5,  8417,   212,    99,    81,  7625,  1466,     5,  8417,\n",
            "          43,     6,   452,  1056,  8062, 25173,  1466,    81,     6,\n",
            "         589,    81,    37,   151,   389,     5,  7332,  4702,     7,\n",
            "        4566,   151,   389,     5,   530,   308,  4702,     7,   572,\n",
            "        4702,  5920,  2198,     5, 21650,  7539,  5005,    41, 31445,\n",
            "           5,  2406,  7811,  4566,     5,  8104, 13918,    57,    57,\n",
            "          57,  5005,    25,  7682,  3385,    61, 12697,  1678,  1674,\n",
            "        4228,  1274,  1855,    73,  1674,    77,   100,  3385,    61,\n",
            "       12697,  1678,    77, 29244,   728,  1678, 13388,   550,    77,\n",
            "         100, 29244,   728, 19838,  3385,    61, 12697,  1678,  1674,\n",
            "        8677, 18498,   197,   704,  2315,  4566,    73,   542,    10,\n",
            "        2229,  1674,  1674,    85,   502,  1674,    77,   100,  3385,\n",
            "          61, 12697,  1678,    77, 29244,   728,  1678, 13388,   266,\n",
            "          77,   100, 29244,   728, 19838,  3385,    61, 12697,  1678,\n",
            "        1674, 21754,  1222,    73,  1674,    77,   100,  3385,    61,\n",
            "       12697,  1678,    77, 29244,   728,  1678, 13388,   266,    77,\n",
            "         100, 29244,   728, 19838,  3385,    61, 12697,  1678,  1674,\n",
            "        6029,    10,  1126,  1782,   313,  1674,    77,   100,  3385,\n",
            "          61, 12697,  1678,    77, 29244,   728,  1678, 13388,   131,\n",
            "          77,   100, 29244,   728, 19838,  3385,    61, 12697,  1678,\n",
            "        1674,  6029,    10,  1126,  1782,   313,  1674,    77,   100,\n",
            "        3385,    61, 12697,  1678,    77, 29244,   728,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'\"@Inject(at = @At(value = \"\"JUMP\"\", ordinal = 8), method = \"\"setupServer()Z\"\") // TODO keep ordinal updated private void init(CallbackInfoReturnable<Boolean> ci) { // Register Bukkit Enchantments for (Enchantment enchantment : Registry.ENCHANTMENT) org.bukkit.enchantments.Enchantment.registerEnchantment(new CardboardEnchantment(enchantment)); CraftMagicNumbers.test(); CraftMagicNumbers.setupUnknownModdedMaterials(); MinecraftDedicatedServer thiss = (MinecraftDedicatedServer) (Object) this; ((MinecraftDedicatedServer) (Object) this).setPlayerManager(new DedicatedPlayerManager(thiss, thiss.getRegistryManager(), saveHandler)); Bukkit.setServer(new CraftServer((MinecraftDedicatedServer) (Object) this)); org.spigotmc.SpigotConfig.init(new File(\"\"spigot.yml\"\")); Bukkit.getLogger().info(\"\"Loading Bukkit plugins...\"\"); File pluginsDir = new File(\"\"plugins\"\"); pluginsDir.mkdir(); Bukkit.getPluginManager().registerInterface(JavaPluginLoader.class); CraftServer s = ((CraftServer)Bukkit.getServer()); if (CraftServer.server == null) CraftServer.server = (MinecraftDedicatedServer) (Object) this; s.loadPlugins(); s.enablePlugins(PluginLoadOrder.STARTUP); Bukkit.getLogger().info(\"\"\"\"); }\"', 'targets': array([19037, 26316,   572,  1119,    43,    60,  3038,   572,  3356,\n",
            "          43,  1674,   706, 28723,    57,    57,     3, 19297,    43,\n",
            "         543,   284,   947,    43,  1674,  2076,   591,  1466,  4566,\n",
            "        1225,    57,    57,    81, 19009,  2910,  6731,   422, 19297,\n",
            "        2095,   186,    71,  2128,   572,  3599,   531,  4908,   308,\n",
            "        7682,  4380,  1678, 11477,    81,    24, 19009,  9386,    41,\n",
            "       31445,    41, 25771,  5238,    23,     6, 25771,   834,  1771,\n",
            "        9296,    55,   834,   226, 12846,     5,   454, 24048,  7285,\n",
            "        6914,    81,   793,     5,   243,  2474, 12536,     5, 19178,\n",
            "        1098,  5238,     5, 25771,   834,     5, 15797, 25771,   834,\n",
            "         572,  3687,  3630,  2896, 25771,   834,   572, 19178,  1098,\n",
            "         834,    81,  5005,  7625, 25185,  1700,     7,     5,  1382,\n",
            "        4566,   151,  7625, 25185,  1700,     7,     5,  2076,   591,\n",
            "       10076, 13631,    45,    74, 11624,     7,  4566,   151, 23884,\n",
            "       25173,  1466,    37,     7,    43,     6,   452,  1056,  8062,\n",
            "       25173,  1466,    81,     6,   589,    81,    37,   151,     6,\n",
            "         572,   452,  1056,  8062, 25173,  1466,    81,     6,   589,\n",
            "          81,    37,   322,  2076,  6885,   659,   572,  3687,    41,\n",
            "       25173,  6885,   659,   572,  7122,     7,     3,    37,     7,\n",
            "           5,  2406,  3252,   659,  4566,     3,   702,  1140,    81,\n",
            "        5005,    41, 31445,     5,  2076,  1466,   572,  3687,  7625,\n",
            "        1466, 26120,   452,  1056,  8062, 25173,  1466,    81,     6,\n",
            "         589,    81,    37,    81,  5005,   793,     5,     7,  5599,\n",
            "       11042,   101,    90,     5, 17970, 11042,   612,     5, 18231,\n",
            "         572,  3687,   856, 13918,    57,     7,  5599, 11042,     5,\n",
            "         125,  5104,    57,    57,    81,  5005,    41, 31445,     5,\n",
            "        2406,  7811,  4566,     5,  8104, 13918,    57, 20815,    41,\n",
            "       31445, 15994, 19031,    57,  5005,   856, 15994,  2361,    43,\n",
            "          56,   856, 13918,    57, 24656,     7,    57,    57,  5005,\n",
            "       15994,  2361,     5,   101,   401,  7399,  4566,   151,    41,\n",
            "       31445,     5,  2406,  4702,   659,  4566,     5, 15797,  3640,\n",
            "         572,  5621,  4702,  3601,     5,  2911,  5005,  7625,  1466,\n",
            "         389,    43,     6,   572, 24843,  1466,    81, 31445,     5,\n",
            "        2406,  1466,  4566,  5005,    66,     6, 24843,  1466,     5,\n",
            "        8417,   212,    99,    81,  7625,  1466,     5,  8417,    43,\n",
            "           6,   452,  1056,  8062, 25173,  1466,    81,     6,   589,\n",
            "          81,    37,   151,   389,     5,  7332,  4702,     7,  4566,\n",
            "         151,   389,     5,   530,   308,  4702,     7,   572,  4702,\n",
            "        5920,  2198,     5, 21650,  7539,  5005,    41, 31445,     5,\n",
            "        2406,  7811,  4566,     5,  8104, 13918,    57,    57,    57,\n",
            "        5005,    25,    57,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'MULTI_LOG_INJECTION: \"public Account queryAccount(String priKey, WalletGrpc.WalletBlockingStub blockingStubFull) { byte[] address; ECKey temKey = null; try { BigInteger priK = new BigInteger(priKey, 16); temKey = ECKey.fromPrivate(priK); } catch (Exception ex) { ex.printStackTrace(); } ECKey ecKey = temKey; if (ecKey == null) { String pubKey = loadPubKey(); //04 PubKey[128] if (StringUtils.isEmpty(pubKey)) { <LOG> return null; } byte[] pubKeyAsc = pubKey.getBytes(); byte[] pubKeyHex = Hex.decode(pubKeyAsc); ecKey = ECKey.fromPublicOnly(pubKeyHex); } return grpcQueryAccount(ecKey.getAddress(), blockingStubFull); }<log_message> \"\"Warning: QueryAccount failed, no wallet address !!\"\" </log_message>  <similarity> 0.89 </similarity><log_message> \"\"Warning: QueryAccount failed, no wallet address !!\"\" </log_message>  <similarity> 0.87 </similarity><log_message> \"\"Warning: QueryAccount failed, no wallet address !!\"\" </log_message>  <similarity> 0.85 </similarity><log_message> </log_message>  <similarity> 0.85 </similarity><log_message> </log_message>  <similarity> 0.85 </similarity><log_message> \"\"Warning: QueryAccount failed, no wallet address !!\"\" </log_message>  <similarity> 0.75 </similarity><log_message> \"\"Warning: QueryAccount failed, no wallet address !!\"\" </log_message>  <similarity> 0.71 </similarity>\"', 'inputs': array([   41, 23774,    61, 13620,    61,  2290,   706,  4337, 10083,\n",
            "          73,    48, 12959,  4949,  1147,  3471,   572,   680,    41,\n",
            "        9622,   506,     3, 23190, 30372,     5, 27672, 15398, 14631,\n",
            "       13116, 14631,  6946,    81,    24,   794,  1726,   935,   809,\n",
            "         151,  8504,   506,  6657,   101,   506,    43,    99,   151,\n",
            "         293,    24,  9551,    41,  9622,   469,    43,    56,  9551,\n",
            "         572,  9622,   506,     3,   990,  5005,  6657,   101,   506,\n",
            "          43,  8504,   506,     5,  5362, 11383,   572,  9622,   469,\n",
            "        5005,    25,   569,     6,   161,  1624,    81,    24,  1624,\n",
            "           5,  6981, 25139,  4566,   151,    25,  8504,   506,   191,\n",
            "          90,   506,    43,  6657,   101,   506,   151,    66,     6,\n",
            "        7941,   506,   212,    99,    81,    24,    67,  7037,   506,\n",
            "          43,  1357, 22463,   506,  4566,   151, 19009,  4250, 14208,\n",
            "         506,  1726, 20958,   935,    66,     6,   680,  1455,     5,\n",
            "         784,  3142,   572, 20963,   506,    81,    81,    24,    77,\n",
            "       13620,  1678,    40,    99,   151,    25,   794,  1726,   935,\n",
            "        7037,   506,  1828,    90,    43,  7037,   506,     5,  2406,\n",
            "        2508,  4566,   151,   794,  1726,   935,  7037,   506, 20248,\n",
            "          43,   262,    11,     5,  1240,  3307,   572, 20963,   506,\n",
            "        1828,    90,  5005,   191,    90,   506,    43,  8504,   506,\n",
            "           5,  5362,  9688,  4945,   572, 20963,   506, 20248,  5005,\n",
            "          25,    40, 30408,   975,  3471,   572,  7941,   506,     5,\n",
            "        2406,  1773,  4566,     3, 13116, 14631,  6946,  5005,    25,\n",
            "        7682,  3385,    61, 12697,  1678,  1674, 11033,    73,  2817,\n",
            "        3471,  2958,     3,   167,  8621,   809,   510,  5470,    57,\n",
            "          77,   100,  3385,    61, 12697,  1678,    77, 29244,   728,\n",
            "        1678, 25149,    59,    77,   100, 29244,   728, 19838,  3385,\n",
            "          61, 12697,  1678,  1674, 11033,    73,  2817,  3471,  2958,\n",
            "           3,   167,  8621,   809,   510,  5470,    57,    77,   100,\n",
            "        3385,    61, 12697,  1678,    77, 29244,   728,  1678, 25149,\n",
            "         585,    77,   100, 29244,   728, 19838,  3385,    61, 12697,\n",
            "        1678,  1674, 11033,    73,  2817,  3471,  2958,     3,   167,\n",
            "        8621,   809,   510,  5470,    57,    77,   100,  3385,    61,\n",
            "       12697,  1678,    77, 29244,   728,  1678, 25149,   550,    77,\n",
            "         100, 29244,   728, 19838,  3385,    61, 12697,  1678,    77,\n",
            "         100,  3385,    61, 12697,  1678,    77, 29244,   728,  1678,\n",
            "       25149,   550,    77,   100, 29244,   728, 19838,  3385,    61,\n",
            "       12697,  1678,    77,   100,  3385,    61, 12697,  1678,    77,\n",
            "       29244,   728,  1678, 25149,   550,    77,   100, 29244,   728,\n",
            "       19838,  3385,    61, 12697,  1678,  1674, 11033,    73,  2817,\n",
            "        3471,  2958,     3,   167,  8621,   809,   510,  5470,    57,\n",
            "          77,   100,  3385,    61, 12697,  1678,    77, 29244,   728,\n",
            "        1678, 23187,   550,    77,   100, 29244,   728, 19838,  3385,\n",
            "          61, 12697,  1678,  1674, 11033,    73,  2817,  3471,  2958,\n",
            "           3,   167,  8621,   809,   510,  5470,    57,    77,   100,\n",
            "        3385,    61, 12697,  1678,    77, 29244,   728,  1678, 23187,\n",
            "         168,    77,   100, 29244,   728,  6628,     1], dtype=int32), 'targets_pretokenized': b'\"public Account queryAccount(String priKey, WalletGrpc.WalletBlockingStub blockingStubFull) { byte[] address; ECKey temKey = null; try { BigInteger priK = new BigInteger(priKey, 16); temKey = ECKey.fromPrivate(priK); } catch (Exception ex) { ex.printStackTrace(); } ECKey ecKey = temKey; if (ecKey == null) { String pubKey = loadPubKey(); //04 PubKey[128] if (StringUtils.isEmpty(pubKey)) { logger.warn(\"\"Warning: QueryAccount failed, no wallet address !!\"\"); return null; } byte[] pubKeyAsc = pubKey.getBytes(); byte[] pubKeyHex = Hex.decode(pubKeyAsc); ecKey = ECKey.fromPublicOnly(pubKeyHex); } return grpcQueryAccount(ecKey.getAddress(), blockingStubFull); }\"', 'targets': array([   48, 12959,  4949,  1147,  3471,   572,   680,    41,  9622,\n",
            "         506,     3, 23190, 30372,     5, 27672, 15398, 14631, 13116,\n",
            "       14631,  6946,    81,    24,   794,  1726,   935,   809,   151,\n",
            "        8504,   506,  6657,   101,   506,    43,    99,   151,   293,\n",
            "          24,  9551,    41,  9622,   469,    43,    56,  9551,   572,\n",
            "        9622,   506,     3,   990,  5005,  6657,   101,   506,    43,\n",
            "        8504,   506,     5,  5362, 11383,   572,  9622,   469,  5005,\n",
            "          25,   569,     6,   161,  1624,    81,    24,  1624,     5,\n",
            "        6981, 25139,  4566,   151,    25,  8504,   506,   191,    90,\n",
            "         506,    43,  6657,   101,   506,   151,    66,     6,  7941,\n",
            "         506,   212,    99,    81,    24,    67,  7037,   506,    43,\n",
            "        1357, 22463,   506,  4566,   151, 19009,  4250, 14208,   506,\n",
            "        1726, 20958,   935,    66,     6,   680,  1455,     5,   784,\n",
            "        3142,   572, 20963,   506,    81,    81,    24,  9180,     5,\n",
            "        4505,    31, 13918,    57, 11033,    73,  2817,  3471,  2958,\n",
            "           3,   167,  8621,   809,   510,  5470,    57,  5005,    40,\n",
            "          99,   151,    25,   794,  1726,   935,  7037,   506,  1828,\n",
            "          90,    43,  7037,   506,     5,  2406,  2508,  4566,   151,\n",
            "         794,  1726,   935,  7037,   506, 20248,    43,   262,    11,\n",
            "           5,  1240,  3307,   572, 20963,   506,  1828,    90,  5005,\n",
            "         191,    90,   506,    43,  8504,   506,     5,  5362,  9688,\n",
            "        4945,   572, 20963,   506, 20248,  5005,    25,    40, 30408,\n",
            "         975,  3471,   572,  7941,   506,     5,  2406,  1773,  4566,\n",
            "           3, 13116, 14631,  6946,  5005,    25,    57,     1],\n",
            "      dtype=int32)}\n"
          ]
        }
      ],
      "source": [
        "finetuning_task = t5.data.TaskRegistry.get(TASK_NAME)\n",
        "ds = finetuning_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko2jBWdwHpI8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "# Learning rate properties\n",
        "starter_learning_rate = 0.01 #@param {type : \"number\"}\n",
        "end_learning_rate = 0.001 #@param {type : \"number\"}\n",
        "decay_steps = 10000 #@param {type : \"integer\"}\n",
        "\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "     starter_learning_rate,\n",
        "     decay_steps,\n",
        "     end_learning_rate,\n",
        "     power=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL9h9EPCjN2O",
        "outputId": "950c7bde-7f6b-4c6c-dd27-d7110f7caace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function truncated_rsqrt at 0x7cf5247d6680>\n"
          ]
        }
      ],
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular, truncated_rsqrt\n",
        "from t5 import models\n",
        "\n",
        "if scheduler == 'polynomial':\n",
        "  learning_rate_scheduler = learning_rate_fn\n",
        "elif scheduler == 'isr':\n",
        "  learning_rate_scheduler = truncated_rsqrt\n",
        "elif scheduler == 'slanted':\n",
        "  learning_rate_scheduler = slanted_triangular\n",
        "else:\n",
        "  learning_rate_scheduler = 0.001\n",
        "\n",
        "print(learning_rate_scheduler)\n",
        "\n",
        "MODEL_SIZE = \"small\"\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 128, 100),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=FINETUNE_MODEL_DIR,\n",
        "    tpu='local',\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 1024, \"targets\": 1024},\n",
        "    learning_rate_schedule = learning_rate_scheduler,\n",
        "    save_checkpoints_steps=10000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR9nSg6Uw758",
        "outputId": "e7ed420c-3f18-4661-e662-919d992f3c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from checkpoint: gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/model.ckpt-600000\n"
          ]
        }
      ],
      "source": [
        "#체크 포인트가 있으면 로드\n",
        "checkpoint_dir = FINETUNE_MODEL_DIR\n",
        "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "if latest_checkpoint:\n",
        "    print(f\"Resuming from checkpoint: {latest_checkpoint}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vA8rJ-GlW37",
        "outputId": "07a6f7ec-4389-4b2b-b041-3ae170fc9e5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/operative_config.gin\n",
            "ERROR:root:Path not found: gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/operative_config.gin\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2043: TPUConfig.__new__ (from tensorflow_estimator.python.estimator.tpu.tpu_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2059: RunConfig.__init__ (from tensorflow_estimator.python.estimator.tpu.tpu_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_config.py:268: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2096: TPUEstimator.__init__ (from tensorflow_estimator.python.estimator.tpu.tpu_estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:2812: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:2372: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/basic_session_run_hooks.py:686: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "WARNING:absl:Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:999: CheckpointSaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:1014: TPUEstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.tpu.tpu_estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3329: LoggingTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3370: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1416: NanTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:586: SummarySaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py:1176: get_checkpoint_mtimes (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:761: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "TPU device does not support heartbeats. Failure handling will be disabled.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1474: SessionRunValues.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n"
          ]
        }
      ],
      "source": [
        "TRAIN_STEPS =  100000 #@param {type: \"integer\"}\n",
        "\n",
        "# Stat finetuning\n",
        "model.finetune(mixture_or_task_name=MIXTURE_NAME,\n",
        "               finetune_steps=TRAIN_STEPS,\n",
        "               pretrained_model_dir=FINETUNE_MODEL_DIR)#PRETRAIN_MODEL_DIR) #FINETUNE_MODEL_DIR\n",
        "               #PRETRAIN_MODEL_DIR: 체크포인트 없을 때\n",
        "               #FINETUNE_MODEL_DIR: 체크포인트 있을 때"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU0o-s4SmxO0",
        "outputId": "6b99a84a-bdf2-4995-d533-475ac3a5b567"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[500000,\n",
              " 510000,\n",
              " 520000,\n",
              " 530000,\n",
              " 540000,\n",
              " 550000,\n",
              " 560000,\n",
              " 570000,\n",
              " 580000,\n",
              " 590000]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoints = []\n",
        "for check in range(500000, 600000, 10000):\n",
        "  checkpoints.append(check)\n",
        "checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnlbTbTOVr8t",
        "outputId": "4b2e0bd5-a7a3-4922-efaa-e9a7c383e991"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/operative_config.gin\n",
            "ERROR:root:Path not found: gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/operative_config.gin\n",
            "INFO:absl:Adding task 'log_injection' with predict metric_fn(s).\n",
            "INFO:absl:Skipping packing/padding for 'log_injection' since sequence length is None.\n",
            "INFO:absl:Setting sequence lengths to {'inputs': 1377, 'targets': 1298}\n",
            "INFO:absl:Evaluating checkpoint step: 600000\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2043: TPUConfig.__new__ (from tensorflow_estimator.python.estimator.tpu.tpu_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2059: RunConfig.__init__ (from tensorflow_estimator.python.estimator.tpu.tpu_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_config.py:268: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2096: TPUEstimator.__init__ (from tensorflow_estimator.python.estimator.tpu.tpu_estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:2812: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:2372: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/basic_session_run_hooks.py:686: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "INFO:absl:Padding 'log_injection' with sequence lengths: {'inputs': 1377, 'targets': 1298}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:824: TPUEstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.tpu.tpu_estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/model_utils/export_utils.py:365: PredictOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3540: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:828: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1474: SessionRunValues.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "INFO:absl:eval/log_injection/accuracy at step 600000: 28.036\n"
          ]
        }
      ],
      "source": [
        "from t5 import models\n",
        "\n",
        "model = t5.models.MtfModel(FINETUNE_MODEL_DIR, tpu='local',sequence_length={\"inputs\": 1024, \"targets\": 1024},\n",
        "    learning_rate_schedule = learning_rate_scheduler,)\n",
        "model.batch_size=16\n",
        "model.eval(\n",
        "    mixture_or_task_name=MIXTURE_NAME,\n",
        "    checkpoint_steps=checkpoints\n",
        "    split='test'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUA8KACV1Cmp",
        "outputId": "35cee170-b717-49df-f50b-e1a1b3640eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/test_eval/log_injection_*_predictions\n",
            "['gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/test_eval/log_injection_600000_predictions']\n",
            "<== Random predictions for finetuning using checkpoint 600000 ==>\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"@Override public void didFinishLaunching() { if (ERXProperties.booleanForKey(\"\"er.javamail.mailer.ERTestSendingMail\"\")); testSendingMail(); int frequency = ERXProperties.intForKey(\"\"er.javamail.mailer.ERBatchMailerDaemonFrequency\"\"); if (frequency > 0) { <LOG> mailTimer = new Timer(true); mailTimer.schedule(new ERMailerTimerTask(), frequency*1000l, frequency*1000l); } else { ERMailer.instance().processOutgoingMail(); <LOG> System.exit(0); } }<log_message> \"\"Creating poll-guard timer with interval [\"\" \"\"ms] while starting SpringJmsConnector\"\" </log_message>  <similarity> 0.24 </similarity><log_message> \"\"Preparing to shutdown simulator...\"\" </log_message>  <similarity> 0.24 </similarity><log_message> \"\"Umbra hook {} ended with exit value {}\"\" </log_message>  <similarity> 0.23 </similarity><log_message> \"\"Umbra hook {} ended with exit value {}\"\" </log_message>  <similarity> 0.23 </similarity><log_message> </log_message>  <similarity> 0.21 </similarity><log_message> \"\"Heritrix3 engine shutdown failed. ExitValue = {}\"\" </log_message>  <similarity> 0.21 </similarity><log_message> \"\"Heritrix3 engine shutdown was successful. ExitValue = {}\"\" </log_message>  <similarity> 0.21 </similarity>\"\n",
            "Target: \"@Override public void didFinishLaunching() { if (ERXProperties.booleanForKey(\"\"er.javamail.mailer.ERTestSendingMail\"\")); testSendingMail(); int frequency = ERXProperties.intForKey(\"\"er.javamail.mailer.ERBatchMailerDaemonFrequency\"\"); if (frequency > 0) { log.debug(\"\"Scheduling timer for frequency: \"\" + frequency + \"\"(s)\"\"); mailTimer = new Timer(true); mailTimer.schedule(new ERMailerTimerTask(), frequency*1000l, frequency*1000l); } else { ERMailer.instance().processOutgoingMail(); log.debug(\"\"Done processing mail. Exiting.\"\"); System.exit(0); } }\"\n",
            "Prediction: \"@Override public void didFinishLaunching() { if (ERXProperties.booleanForKey(\"\"er.javamail.mailer.ERTestSendingMail\"\")); testSendingMail(); int frequency = ERXProperties.intForKey(\"\"er.javamail.mailer.ERBatchMailerDaemonFrequency\"\"); if (frequency > 0) { log.info(\"\"Scheduling mailer daemon frequency to run every {} seconds\"\", frequency); mailTimer = new Timer(true); mailTimer.schedule(new ERMailerTimerTask(), frequency*1000l, frequency*1000l); } else { ERMailer.instance().processOutgoingMail(); log.info(\"\"Finished sending mail\"\"); System.exit(0); } }\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"@Override public void flush(Context context, String chromosome, int position) throws IOException, InterruptedException { <LOG> variantsInBatch = 0; for (SampleIndexEntryPutBuilder builder : samplesMap.values()) { Put put = builder.build(); if (put == null || put.isEmpty()) { context.getCounter(COUNTER_GROUP_NAME, \"\"empty_put\"\").increment(1); } else { context.write(new ImmutableBytesWritable(put.getRow()), put); } } samplesMap.clear(); }<log_message> \"\"Derby log: {}\"\" </log_message>  <similarity> 0.30 </similarity><log_message> \"\"Derby log: {}\"\" </log_message>  <similarity> 0.30 </similarity><log_message> \"\"Derby log: {}\"\" </log_message>  <similarity> 0.30 </similarity><log_message> \"\"Derby log: {}\"\" </log_message>  <similarity> 0.30 </similarity><log_message> \"\"Skipped record: \"\" </log_message>  <similarity> 0.30 </similarity><log_message> \"\"reduce: key[\"\" \"\"] -> \"\" \"\", value -> \"\" </log_message>  <similarity> 0.29 </similarity><log_message> \"\"Problem with variant \"\" </log_message>  <similarity> 0.27 </similarity>\"\n",
            "Target: \"@Override public void flush(Context context, String chromosome, int position) throws IOException, InterruptedException { logger.info(\"\"Flush {}:{}-{} with {} variants\"\", chromosome, SampleIndexSchema.getChunkStart(position), SampleIndexSchema.getChunkStartNext(position), variantsInBatch); variantsInBatch = 0; for (SampleIndexEntryPutBuilder builder : samplesMap.values()) { Put put = builder.build(); if (put == null || put.isEmpty()) { context.getCounter(COUNTER_GROUP_NAME, \"\"empty_put\"\").increment(1); } else { context.write(new ImmutableBytesWritable(put.getRow()), put); } } samplesMap.clear(); }\"\n",
            "Prediction: \"@Override public void flush(Context context, String chromosome, int position) throws IOException, InterruptedException { LOG.info(\"\"Flushing \"\" + variantsInBatch + \"\" samples\"\"); variantsInBatch = 0; for (SampleIndexEntryPutBuilder builder : samplesMap.values()) { Put put = builder.build(); if (put == null || put.isEmpty()) { context.getCounter(COUNTER_GROUP_NAME, \"\"empty_put\"\").increment(1); } else { context.write(new ImmutableBytesWritable(put.getRow()), put); } } samplesMap.clear(); }\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"partition -> <LOG><log_message> \"\"TopicPartition \\\"\" \"\": starting offset: {}, stopping offset: {}\"\" </log_message>  <similarity> 1.00 </similarity><log_message> \"\"TopicPartition \\\"\" \"\": starting offset: {}, stopping offset: {}\"\" </log_message>  <similarity> 1.00 </similarity><log_message> \"\"TopicPartition \\\"\" \"\": starting offset: {}, stopping offset: {}\"\" </log_message>  <similarity> 1.00 </similarity><log_message> \"\"Rebalance happened \"\" \"\":\"\" </log_message>  <similarity> 0.33 </similarity><log_message> \"\"Receive add published partition to txn request {} \"\" \"\"from {} with txnId {}, topic: [{}]\"\" </log_message>  <similarity> 0.33 </similarity>\"\n",
            "Target: \"partition -> logger.debug(\"\"{} is {} partition in helix only\"\", partition, partitionStateStr));\"\n",
            "Prediction: \"partition -> LOG.info( \"\"TopicPartition \\\"\"{}\\\"\": starting offset: {}, stopping offset: {}\"\", partition, beginningOffsets.get(partition), stoppingOffsets.get(partition)));\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"@Override public void enlist(Transaction transaction, LocalTransaction ltx) { LocalXaTransaction localTransaction = (LocalXaTransaction) ltx; if (!localTransaction.isEnlisted()) { //make sure that you only enlist it once try { transaction.enlistResource(new TransactionXaAdapter( localTransaction, this, recoveryManager, txCoordinator, commandsFactory, rpcManager, clusteringLogic, configuration, cacheName, partitionHandlingManager)); } catch (Exception e) { Xid xid = localTransaction.getXid(); if (xid != null && !localTransaction.getLookedUpEntries().isEmpty()) { <LOG> try { txCoordinator.rollback(localTransaction); } catch (XAException xae) { <LOG> } } log.failedToEnlistTransactionXaAdapter(e); throw new CacheException(e); } } }<log_message> \"\"Attempting a rollback to clear stale resources!\"\" </log_message>  <similarity> 0.98 </similarity><log_message> \"\"Caught exception attempting to clean up \"\" </log_message>  <similarity> 0.98 </similarity><log_message> \"\"Attempting a rollback to clear stale resources asynchronously!\"\" </log_message>  <similarity> 0.73 </similarity><log_message> \"\"Caught exception attempting to clean up \"\" \"\" for \"\" </log_message>  <similarity> 0.73 </similarity><log_message> \"\"Recovery not enabled\"\" </log_message>  <similarity> 0.25 </similarity><log_message> \"\"Recovery not enabled\"\" </log_message>  <similarity> 0.24 </similarity><log_message> \"\"XA resource rollback(...) [xid=\"\" \"\"]\"\" </log_message>  <similarity> 0.24 </similarity>\"\n",
            "Target: \"@Override public void enlist(Transaction transaction, LocalTransaction ltx) { LocalXaTransaction localTransaction = (LocalXaTransaction) ltx; if (!localTransaction.isEnlisted()) { //make sure that you only enlist it once try { transaction.enlistResource(new TransactionXaAdapter( localTransaction, this, recoveryManager, txCoordinator, commandsFactory, rpcManager, clusteringLogic, configuration, cacheName, partitionHandlingManager)); } catch (Exception e) { Xid xid = localTransaction.getXid(); if (xid != null && !localTransaction.getLookedUpEntries().isEmpty()) { log.debug(\"\"Attempting a rollback to clear stale resources!\"\"); try { txCoordinator.rollback(localTransaction); } catch (XAException xae) { log.debug(\"\"Caught exception attempting to clean up \"\" + xid, xae); } } log.failedToEnlistTransactionXaAdapter(e); throw new CacheException(e); } } }\"\n",
            "Prediction: \"@Override public void enlist(Transaction transaction, LocalTransaction ltx) { LocalXaTransaction localTransaction = (LocalXaTransaction) ltx; if (!localTransaction.isEnlisted()) { //make sure that you only enlist it once try { transaction.enlistResource(new TransactionXaAdapter( localTransaction, this, recoveryManager, txCoordinator, commandsFactory, rpcManager, clusteringLogic, configuration, cacheName, partitionHandlingManager)); } catch (Exception e) { Xid xid = localTransaction.getXid(); if (xid != null && !localTransaction.getLookedUpEntries().isEmpty()) { log.debug(\"\"Attempting a rollback to clear stale resources!\"\"); try { txCoordinator.rollback(localTransaction); } catch (XAException xae) { log.debug(\"\"Caught exception attempting to clean up \"\" + xid, xae); } } log.failedToEnlistTransactionXaAdapter(e); throw new CacheException(e); } } }\"\n",
            "Counted as Correct? True\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"private void cancelKillMapJobs(KillMapType killmapType, List<Integer> ids) { if (KillmapDAO.removeKillmapJobsByIds(killmapType, ids)) { <LOG> messages.add(String.format(\"\"Successfully canceled %d %s.\"\", ids.size(), pluralize(ids.size(), \"\"job\"\", \"\"jobs\"\"))); } else { <LOG> messages.add(\"\"Failed to cancel selected jobs.\"\"); } }<log_message> \"\"User {} deleted killmaps for {}: {}\"\" \"\", \"\" </log_message>  <similarity> 0.37 </similarity><log_message> \"\"Failed to delete killmaps: {}\"\" \"\", \"\" </log_message>  <similarity> 0.37 </similarity><log_message> \"\"Unknown killmap type: \"\" </log_message>  <similarity> 0.22 </similarity><log_message> \"\"Failed to queue killmap for {}: {}\"\" </log_message>  <similarity> 0.22 </similarity><log_message> \"\"User {} queued killmaps for {}: {}\"\" </log_message>  <similarity> 0.22 </similarity><log_message> \"\"Unknown killmap type: \"\" </log_message>  <similarity> 0.22 </similarity><log_message> \"\"Configuring job {}/{} for updating API specifications.\"\" </log_message>  <similarity> 0.21 </similarity><log_message> \"\"Job {}/{} is not configured to be enabled, therefore it will not be activated.\"\" </log_message>  <similarity> 0.21 </similarity>\"\n",
            "Target: \"private void cancelKillMapJobs(KillMapType killmapType, List<Integer> ids) { if (KillmapDAO.removeKillmapJobsByIds(killmapType, ids)) { logger.info(\"\"User {} canceled killmap jobs for {}: {}\"\", login.getUserId(), killmapType, StringUtils.join(ids, \"\", \"\")); messages.add(String.format(\"\"Successfully canceled %d %s.\"\", ids.size(), pluralize(ids.size(), \"\"job\"\", \"\"jobs\"\"))); } else { logger.warn(\"\"Failed to cancel killmap jobs: \"\" + StringUtils.join(ids, \"\", \"\")); messages.add(\"\"Failed to cancel selected jobs.\"\"); } }\"\n",
            "Prediction: \"private void cancelKillMapJobs(KillMapType killmapType, List<Integer> ids) { if (KillmapDAO.removeKillmapJobsByIds(killmapType, ids)) { LOG.info(\"\"User {} canceled killmaps for {}: {}\"\", login.getUserId(), killmapType.name(), StringUtils.join(ids, \"\", \"\")); messages.add(String.format(\"\"Successfully canceled %d %s.\"\", ids.size(), pluralize(ids.size(), \"\"job\"\", \"\"jobs\"\"))); } else { LOG.warn(\"\"Failed to cancel selected jobs: {}\"\", StringUtils.join(ids, \"\", \"\")); messages.add(\"\"Failed to cancel selected jobs.\"\"); } }\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"@Override public void addCategory(Category category) { Connection conn = null; PreparedStatement stat = null; try { conn = this.getConnection(); conn.setAutoCommit(false); stat = conn.prepareStatement(this.getAddCategoryQuery()); stat.setString(1, category.getCode()); stat.setString(2, category.getParentCode()); stat.setString(3, category.getTitles().toXml()); stat.executeUpdate(); conn.commit(); } catch (Throwable t) { this.executeRollback(conn); <LOG> throw new RuntimeException(\"\"Error while inserting a new category\"\", t); //processDaoException(t, \"\"Error while inserting a new category\"\", \"\"addCategory\"\"); } finally { closeDaoResources(null, stat, conn); } }<log_message> \"\"Error detected while updating a category\"\" </log_message>  <similarity> 0.85 </similarity><log_message> \"\"Error detected while updating a category\"\" </log_message>  <similarity> 0.85 </similarity><log_message> \"\"Error detected while deleting category '{}'\"\" </log_message>  <similarity> 0.67 </similarity><log_message> \"\"Error detected while deleting category '{}'\"\" </log_message>  <similarity> 0.67 </similarity><log_message> \"\"Error while adding a permission\"\" </log_message>  <similarity> 0.64 </similarity>\"\n",
            "Target: \"@Override public void addCategory(Category category) { Connection conn = null; PreparedStatement stat = null; try { conn = this.getConnection(); conn.setAutoCommit(false); stat = conn.prepareStatement(this.getAddCategoryQuery()); stat.setString(1, category.getCode()); stat.setString(2, category.getParentCode()); stat.setString(3, category.getTitles().toXml()); stat.executeUpdate(); conn.commit(); } catch (Throwable t) { this.executeRollback(conn); _logger.error(\"\"Error while inserting a new category\"\", t); throw new RuntimeException(\"\"Error while inserting a new category\"\", t); //processDaoException(t, \"\"Error while inserting a new category\"\", \"\"addCategory\"\"); } finally { closeDaoResources(null, stat, conn); } }\"\n",
            "Prediction: \"@Override public void addCategory(Category category) { Connection conn = null; PreparedStatement stat = null; try { conn = this.getConnection(); conn.setAutoCommit(false); stat = conn.prepareStatement(this.getAddCategoryQuery()); stat.setString(1, category.getCode()); stat.setString(2, category.getParentCode()); stat.setString(3, category.getTitles().toXml()); stat.executeUpdate(); conn.commit(); } catch (Throwable t) { this.executeRollback(conn); _logger.error(\"\"Error while inserting a new category\"\", t); throw new RuntimeException(\"\"Error while inserting a new category\"\", t); //processDaoException(t, \"\"Error while inserting a new category\"\", \"\"addCategory\"\"); } finally { closeDaoResources(null, stat, conn); } }\"\n",
            "Counted as Correct? True\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"private int compareDateStrings(String first, String second) { try { OffsetDateTime firstDate = parseDate(first, DOCKER_DATE_FORMAT); OffsetDateTime secondDate = parseDate(second, DOCKER_DATE_FORMAT); OffsetDateTime hourEarlier = firstDate.minusHours(1); OffsetDateTime hourLater = firstDate.plusHours(1); boolean secondIsWithinAnHourOfFirst = hourEarlier.isBefore(secondDate) && hourLater.isAfter(secondDate); if (secondIsWithinAnHourOfFirst) { return 0; } if (firstDate.isAfter(secondDate)) { return -1; } else if (firstDate.isBefore(secondDate)) { return 1; } } catch (ParseException e) { <LOG> } return 0; }<log_message> \"\"Could not retrieve ContainerInfo from container manager for \"\" \"\"comparison.\"\" </log_message>  <similarity> 0.31 </similarity><log_message> \"\"Badly formatted error code '{}' received from device: {}\"\" </log_message>  <similarity> 0.28 </similarity><log_message> \"\"invalid entry, must be number\"\" </log_message>  <similarity> 0.28 </similarity><log_message> \"\"An error occurred while retrieving the active session count. Therefore the active session \"\" \"\"count is set to -1 in the analytics event.\"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"\"\" </log_message>  <similarity> 0.27 </similarity>\"\n",
            "Target: \"private int compareDateStrings(String first, String second) { try { OffsetDateTime firstDate = parseDate(first, DOCKER_DATE_FORMAT); OffsetDateTime secondDate = parseDate(second, DOCKER_DATE_FORMAT); OffsetDateTime hourEarlier = firstDate.minusHours(1); OffsetDateTime hourLater = firstDate.plusHours(1); boolean secondIsWithinAnHourOfFirst = hourEarlier.isBefore(secondDate) && hourLater.isAfter(secondDate); if (secondIsWithinAnHourOfFirst) { return 0; } if (firstDate.isAfter(secondDate)) { return -1; } else if (firstDate.isBefore(secondDate)) { return 1; } } catch (ParseException e) { logger.debug(String.format(\"\"Could not parse the date strings with the format %s.\"\", DOCKER_DATE_FORMAT), e); } return 0; }\"\n",
            "Prediction: \"private int compareDateStrings(String first, String second) { try { OffsetDateTime firstDate = parseDate(first, DOCKER_DATE_FORMAT); OffsetDateTime secondDate = parseDate(second, DOCKER_DATE_FORMAT); OffsetDateTime hourEarlier = firstDate.minusHours(1); OffsetDateTime hourLater = firstDate.plusHours(1); boolean secondIsWithinAnHourOfFirst = hourEarlier.isBefore(secondDate) && hourLater.isAfter(secondDate); if (secondIsWithinAnHourOfFirst) { return 0; } if (firstDate.isAfter(secondDate)) { return -1; } else if (firstDate.isBefore(secondDate)) { return 1; } } catch (ParseException e) { LOG.warn(\"\"Could not compare date strings: \"\" + e.getMessage()); } return 0; }\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"@Path(\"\"to/{name}\"\") @GET @Produces(MediaType.APPLICATION_JSON) public Response helloTo(@PathParam(\"\"name\"\") String name) { JSONObject response = new JSONObject(); try { response.put(\"\"Message\"\", \"\"Hello \"\" + name + \"\" from Netflix OSS\"\"); return Response.ok(response.toString()).build(); } catch (JSONException e) { <LOG> return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build(); } }<log_message> \"\"Error creating json response.\"\" </log_message>  <similarity> 0.83 </similarity><log_message> \"\"Error creating json response.\"\" </log_message>  <similarity> 0.79 </similarity><log_message> \"\"Get All Edges.\"\" </log_message>  <similarity> 0.45 </similarity><log_message> \"\"Get All Vertices\"\" </log_message>  <similarity> 0.45 </similarity><log_message> \"\"Series ACL lookup: {}\"\" </log_message>  <similarity> 0.42 </similarity><log_message> \"\"Could not retrieve series ACL: {}\"\" </log_message>  <similarity> 0.42 </similarity>\"\n",
            "Target: \"@Path(\"\"to/{name}\"\") @GET @Produces(MediaType.APPLICATION_JSON) public Response helloTo(@PathParam(\"\"name\"\") String name) { JSONObject response = new JSONObject(); try { response.put(\"\"Message\"\", \"\"Hello \"\" + name + \"\" from Netflix OSS\"\"); return Response.ok(response.toString()).build(); } catch (JSONException e) { logger.error(\"\"Error creating json response.\"\", e); return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build(); } }\"\n",
            "Prediction: \"@Path(\"\"to/{name}\"\") @GET @Produces(MediaType.APPLICATION_JSON) public Response helloTo(@PathParam(\"\"name\"\") String name) { JSONObject response = new JSONObject(); try { response.put(\"\"Message\"\", \"\"Hello \"\" + name + \"\" from Netflix OSS\"\"); return Response.ok(response.toString()).build(); } catch (JSONException e) { logger.error(\"\"Error creating json response.\"\", e); return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build(); } }\"\n",
            "Counted as Correct? True\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"@RequestMapping(value = \"\"/routes/all\"\", method = RequestMethod.GET) public String getAllRoutes() { <LOG> try { List<RouteModel> proxies = service.getAllRoutes(); return pretty.encode(proxies); } catch (Exception e) { <LOG> return pretty.encode(RetMessage.createFailMessage(e.getMessage())); } }<log_message> \"\"[getActiveRoutes]\"\" </log_message>  <similarity> 0.85 </similarity><log_message> \"\"[getAllProxies]\"\" </log_message>  <similarity> 0.81 </similarity><log_message> \"\"[getActiveProxies]\"\" </log_message>  <similarity> 0.74 </similarity><log_message> \"\"[addRoute] add one: {}\"\" </log_message>  <similarity> 0.45 </similarity><log_message> \"\"[addRoute]\"\" </log_message>  <similarity> 0.45 </similarity><log_message> \"\"[updateRoute] updated one: {}\"\" </log_message>  <similarity> 0.45 </similarity><log_message> \"\"[updateRoute]\"\" </log_message>  <similarity> 0.45 </similarity>\"\n",
            "Target: \"@RequestMapping(value = \"\"/routes/all\"\", method = RequestMethod.GET) public String getAllRoutes() { logger.info(\"\"[getAllRoutes][begin]\"\"); try { List<RouteModel> proxies = service.getAllRoutes(); return pretty.encode(proxies); } catch (Exception e) { logger.error(\"\"[getAllRoutes]\"\", e); return pretty.encode(RetMessage.createFailMessage(e.getMessage())); } }\"\n",
            "Prediction: \"@RequestMapping(value = \"\"/routes/all\"\", method = RequestMethod.GET) public String getAllRoutes() { logger.info(\"\"[getAllRoutes]\"\"); try { List<RouteModel> proxies = service.getAllRoutes(); return pretty.encode(proxies); } catch (Exception e) { logger.error(\"\"[getAllRoutes]\"\", e); return pretty.encode(RetMessage.createFailMessage(e.getMessage())); } }\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: MULTI_LOG_INJECTION: \"protected BlobContainerClient createBlobStore(String partitionId) { BlobContainerClient blobContainerClient; try { blobContainerClient = azureStorageClient.createBlobContainer(String.valueOf(partitionId)); } catch (BlobStorageException blobStorageException) { if (blobStorageException.getErrorCode().equals(BlobErrorCode.CONTAINER_ALREADY_EXISTS)) { <LOG> return getBlobStore(partitionId); } azureMetrics.blobContainerErrorCount.inc(); <LOG> throw blobStorageException; } return blobContainerClient; }<log_message> \"\"Azure blob storage container for partition {} does not exist\"\" </log_message>  <similarity> 0.61 </similarity><log_message> \"\"Failed to get Azure blob storage container for partition {} due to {}\"\" </log_message>  <similarity> 0.61 </similarity><log_message> \"\"Creating container {}.\"\" </log_message>  <similarity> 0.31 </similarity><log_message> \"\"Could not parse value of FlowLog message to Integer.\"\" </log_message>  <similarity> 0.21 </similarity><log_message> \"\"Could not parse value of FlowLog message to Long.\"\" </log_message>  <similarity> 0.21 </similarity>\"\n",
            "Target: \"protected BlobContainerClient createBlobStore(String partitionId) { BlobContainerClient blobContainerClient; try { blobContainerClient = azureStorageClient.createBlobContainer(String.valueOf(partitionId)); } catch (BlobStorageException blobStorageException) { if (blobStorageException.getErrorCode().equals(BlobErrorCode.CONTAINER_ALREADY_EXISTS)) { logger.info(\"\"Azure blob storage container for partition {} already exists\"\", partitionId); return getBlobStore(partitionId); } azureMetrics.blobContainerErrorCount.inc(); logger.error(\"\"Failed to create Azure blob storage container for partition {} due to {}\"\", partitionId, blobStorageException.getServiceMessage()); throw blobStorageException; } return blobContainerClient; }\"\n",
            "Prediction: \"protected BlobContainerClient createBlobStore(String partitionId) { BlobContainerClient blobContainerClient; try { blobContainerClient = azureStorageClient.createBlobContainer(String.valueOf(partitionId)); } catch (BlobStorageException blobStorageException) { if (blobStorageException.getErrorCode().equals(BlobErrorCode.CONTAINER_ALREADY_EXISTS)) { logger.info(\"\"Azure blob storage container for partition {} does not exist\"\", partitionId); return getBlobStore(partitionId); } azureMetrics.blobContainerErrorCount.inc(); logger.error(\"\"Failed to create Azure blob storage container for partition {} due to {}\"\", partitionId, blobStorageException.getMessage()); throw blobStorageException; } return blobContainerClient; }\"\n",
            "Counted as Correct? False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#MULTI-LOG INJECITON 예시출력용\n",
        "import random\n",
        "\n",
        "def print_random_predictions(task_name, TASK_NAME, my_finetuning_task, n=10):\n",
        "  \"\"\"Print n predictions from the validation split of a task.\"\"\"\n",
        "  # Grab the dataset for this task.\n",
        "  finetuning_task = t5.data.TaskRegistry.get(TASK_NAME)\n",
        "  ds = finetuning_task.get_dataset(split=\"test\", sequence_length={\"inputs\": 1024, \"targets\": 1024},shuffle=False)\n",
        "\n",
        "  def _prediction_file_to_ckpt(path):\n",
        "    \"\"\"Extract the global step from a prediction filename.\"\"\"\n",
        "    return int(path.split(\"_\")[-2])\n",
        "\n",
        "  # Grab the paths of all logged predictions.\n",
        "  prediction_files = tf.io.gfile.glob(\n",
        "      os.path.join(\n",
        "          FINETUNE_MODEL_DIR,\n",
        "          \"test_eval/%s_*_predictions\" % TASK_NAME ))\n",
        "  print(os.path.join(\n",
        "          FINETUNE_MODEL_DIR,\n",
        "          \"test_eval/%s_*_predictions\" % TASK_NAME))\n",
        "  print(prediction_files)\n",
        " # Get most recent prediction file by sorting by their step.\n",
        "  latest_prediction_file = sorted(\n",
        "      prediction_files, key=_prediction_file_to_ckpt)[-1]\n",
        "\n",
        "  # Collect (inputs, targets, prediction) from the dataset and predictions file\n",
        "  results = []\n",
        "  with tf.io.gfile.GFile(latest_prediction_file) as preds:\n",
        "    for ex, pred in zip(tfds.as_numpy(ds), preds):\n",
        "      results.append((tf.compat.as_text(ex[\"inputs_pretokenized\"]),\n",
        "                      tf.compat.as_text(ex[\"targets_pretokenized\"]),\n",
        "                      pred.strip()))\n",
        "\n",
        "  print(\"<== Random predictions for %s using checkpoint %s ==>\\n\" %\n",
        "        (task_name,\n",
        "         _prediction_file_to_ckpt(latest_prediction_file)))\n",
        "\n",
        "  for inp, tgt, pred in random.choices(results, k=10):\n",
        "    print(\"Input:\", inp)\n",
        "    print(\"Target:\", tgt)\n",
        "    print(\"Prediction:\", pred)\n",
        "    print(\"Counted as Correct?\", tgt == pred)\n",
        "    print()\n",
        "\n",
        "print_random_predictions(MIXTURE_NAME,TASK_NAME,prefix)\n",
        "#print_random_predictions(\"nq_context_free\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGm1zekZyryj",
        "outputId": "99b8733d-8d26-4bd3-8c3b-248d6ea532e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install rouge-score\n",
        "!pip install tree_sitter\n",
        "!pip install requests\n",
        "!pip install tensorflow-text\n",
        "!pip install seqeval\n",
        "!pip install pygments\n",
        "clear_output()\n",
        "print(\"Installing dependencies...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R48hh1M_Tlj",
        "outputId": "d87818a3-70f2-44fe-f46a-8e4af90265c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, GitPython\n",
            "Successfully installed GitPython-3.1.43 gitdb-4.0.11 smmap-5.0.1\n"
          ]
        }
      ],
      "source": [
        "#!git clone https://github.com/tree-sitter/tree-sitter-java.git\n",
        "\n",
        "!pip install GitPython\n",
        "\n",
        "from git import Repo\n",
        "\n",
        "repo_url = 'https://github.com/tree-sitter/tree-sitter-java.git'\n",
        "clone_directory = '/content/CodeBLEU_2/parsers/tree-sitter-java'\n",
        "\n",
        "if not os.path.exists(clone_directory):\n",
        "    Repo.clone_from(repo_url, clone_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiexrCXTzQQ2",
        "outputId": "30b85b5f-5393-4397-891d-bc97d944a3d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text\n",
        "import t5\n",
        "import seqeval\n",
        "\n",
        "sys.path.append('/content/CodeBLEU_2')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENKVLAp3zUH3"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "#from CodeBLEU_2 import calc_code_bleu\n",
        "from pygments.lexers import get_lexer_by_name\n",
        "from pygments.token import Token\n",
        "from pygments import lex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vd0rIpMGXDc"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "def _prediction_file_to_ckpt(path):\n",
        "    \"\"\"Extract the global step from a prediction filename.\"\"\"\n",
        "    return int(path.split(\"_\")[-2])\n",
        "\n",
        "# 예측 결과 파일 경로 패턴\n",
        "prediction_files_pattern = os.path.join(\n",
        "    FINETUNE_MODEL_DIR,\n",
        "    \"test_eval\",\n",
        "    \"%s_*_predictions\" % TASK_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INbDhs8mb24b",
        "outputId": "08087f61-4111-4539-9441-d5b3dca1ea5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사용할 예측 결과 파일: gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/test_eval/log_injection_600000_predictions\n",
            "Input: MULTI_LOG_INJECTION: \"@Override public void doInTransactionWithoutResult(final TransactionStatus status) { if (addVnetsFinal != null) { <LOG> //add vnet takes a list of strings to be added. each string is a vnet. _dcDao.addVnet(network.getDataCenterId(), network.getId(), addVnetsFinal); } if (removeVnetsFinal != null) { <LOG> //deleteVnets takes a list of strings to be removed. each string is a vnet. _datacneterVnet.deleteVnets(TransactionLegacy.currentTxn(), network.getDataCenterId(), network.getId(), removeVnetsFinal); } _physicalNetworkDao.update(network.getId(), network); }<log_message> \"\"Adding vnet range \"\" \"\" for the physicalNetwork id= \"\" \"\" and zone id=\"\" \"\" as a part of updatePhysicalNetwork call\"\" </log_message>  <similarity> 0.89 </similarity><log_message> \"\"removing vnet range \"\" \"\" for the physicalNetwork id= \"\" \"\" and zone id=\"\" \"\" as a part of updatePhysicalNetwork call\"\" </log_message>  <similarity> 0.89 </similarity><log_message> \"\"Adding vnet range \"\" \"\" for the physicalNetwork id= \"\" \"\" and zone id=\"\" \"\" as a part of updatePhysicalNetwork call\"\" </log_message>  <similarity> 0.41 </similarity><log_message> \"\"removing vnet range \"\" \"\" for the physicalNetwork id= \"\" \"\" and zone id=\"\" \"\" as a part of updatePhysicalNetwork call\"\" </log_message>  <similarity> 0.41 </similarity><log_message> \"\"Adding vnet range \"\" \"\" for the physicalNetwork id= \"\" \"\" and zone id=\"\" \"\" as a part of updatePhysicalNetwork call\"\" </log_message>  <similarity> 0.38 </similarity><log_message> \"\"removing vnet range \"\" \"\" for the physicalNetwork id= \"\" \"\" and zone id=\"\" \"\" as a part of updatePhysicalNetwork call\"\" </log_message>  <similarity> 0.38 </similarity><log_message> \"\"Successfully removed firewall rule with ip id=\"\" \"\" and port \"\" \"\" as a part of vpn cleanup\"\" </log_message>  <similarity> 0.27 </similarity><log_message> \"\"Revoving nic secondary ip entry ...\"\" </log_message>  <similarity> 0.24 </similarity>\"\n",
            "Reference: \"@Override public void doInTransactionWithoutResult(final TransactionStatus status) { if (addVnetsFinal != null) { s_logger.debug(\"\"Adding vnet range \"\" + addVnetsFinal.toString() + \"\" for the physicalNetwork id= \"\" + network.getId() + \"\" and zone id=\"\" + network.getDataCenterId() + \"\" as a part of updatePhysicalNetwork call\"\"); //add vnet takes a list of strings to be added. each string is a vnet. _dcDao.addVnet(network.getDataCenterId(), network.getId(), addVnetsFinal); } if (removeVnetsFinal != null) { s_logger.debug(\"\"removing vnet range \"\" + removeVnetsFinal.toString() + \"\" for the physicalNetwork id= \"\" + network.getId() + \"\" and zone id=\"\" + network.getDataCenterId() + \"\" as a part of updatePhysicalNetwork call\"\"); //deleteVnets takes a list of strings to be removed. each string is a vnet. _datacneterVnet.deleteVnets(TransactionLegacy.currentTxn(), network.getDataCenterId(), network.getId(), removeVnetsFinal); } _physicalNetworkDao.update(network.getId(), network); }\"\n",
            "Prediction: \"@Override public void doInTransactionWithoutResult(final TransactionStatus status) { if (addVnetsFinal != null) { logger.debug(\"\"Adding vnet range \"\" + addVnetsFinal + \"\" for the physicalNetwork id= \"\" + network.getId() + \"\" and zone id=\"\" + network.getDataCenterId() + \"\" as a part of updatePhysicalNetwork call\"\"); //add vnet takes a list of strings to be added. each string is a vnet. _dcDao.addVnet(network.getDataCenterId(), network.getId(), addVnetsFinal); } if (removeVnetsFinal != null) { logger.debug(\"\"removing vnet range \"\" + removeVnetsFinal + \"\" for the physicalNetwork id= \"\" + network.getId() + \"\" and zone id=\"\" + network.getDataCenterId() + \"\" as a part of updatePhysicalNetwork call\"\"); //deleteVnets takes a list of strings to be removed. each string is a vnet. _datacneterVnet.deleteVnets(TransactionLegacy.currentTxn(), network.getDataCenterId(), network.getId(), removeVnetsFinal); } _physicalNetworkDao.update(network.getId(), network); }\"\n"
          ]
        }
      ],
      "source": [
        "#input, reference, prediciton 예시 출력\n",
        "prediction_files = tf.io.gfile.glob(prediction_files_pattern)\n",
        "\n",
        "latest_prediction_file = sorted(\n",
        "    prediction_files, key=_prediction_file_to_ckpt)[-1]\n",
        "\n",
        "print(\"사용할 예측 결과 파일:\", latest_prediction_file)\n",
        "\n",
        "finetuning_task = t5.data.TaskRegistry.get(TASK_NAME)\n",
        "ds = finetuning_task.get_dataset(\n",
        "    split=\"test\",\n",
        "    sequence_length={\"inputs\": 1024, \"targets\": 1024},\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "results = []\n",
        "with tf.io.gfile.GFile(latest_prediction_file) as preds:\n",
        "    for ex, pred in zip(tfds.as_numpy(ds), preds):\n",
        "        input_text = tf.compat.as_text(ex[\"inputs_pretokenized\"])\n",
        "        target_text = tf.compat.as_text(ex[\"targets_pretokenized\"])\n",
        "        prediction_text = pred.strip()\n",
        "        results.append((input_text, target_text, prediction_text))\n",
        "\n",
        "inputs, references, predictions = zip(*results)\n",
        "\n",
        "print(\"Input:\", inputs[2])\n",
        "print(\"Reference:\", references[2])\n",
        "print(\"Prediction:\", predictions[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbfJl7nFVFUp",
        "outputId": "5b84de83-0efa-4a09-f43d-625497f261a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사용할 예측 결과 파일: gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/test_eval/log_injection_600000_predictions\n",
            "==== BLEU Scores ====\n",
            "BLEU-1: 0.8288\n",
            "BLEU-2: 0.7673\n",
            "BLEU-3: 0.6907\n",
            "BLEU-4: 0.6491\n"
          ]
        }
      ],
      "source": [
        "#CodeBLEU 점수 측정\n",
        "prediction_files = tf.io.gfile.glob(prediction_files_pattern)\n",
        "\n",
        "latest_prediction_file = sorted(\n",
        "    prediction_files, key=_prediction_file_to_ckpt)[-1]\n",
        "\n",
        "print(\"사용할 예측 결과 파일:\", latest_prediction_file)\n",
        "\n",
        "finetuning_task = t5.data.TaskRegistry.get(TASK_NAME)\n",
        "ds = finetuning_task.get_dataset(\n",
        "    split=\"test\",\n",
        "    sequence_length={\"inputs\": 1024, \"targets\": 1024},\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "results = []\n",
        "with tf.io.gfile.GFile(latest_prediction_file) as preds:\n",
        "    for ex, pred in zip(tfds.as_numpy(ds), preds):\n",
        "        input_text = tf.compat.as_text(ex[\"inputs_pretokenized\"])\n",
        "        target_text = tf.compat.as_text(ex[\"targets_pretokenized\"])\n",
        "        prediction_text = pred.strip()\n",
        "        results.append((input_text, target_text, prediction_text))\n",
        "\n",
        "inputs, references, predictions = zip(*results)\n",
        "\n",
        "inputs = list(inputs)\n",
        "references = list(references)\n",
        "predictions = list(predictions)\n",
        "\n",
        "#def tokenize_code(code):\n",
        "#    return nltk.word_tokenize(code)\n",
        "\n",
        "def tokenize_code(code, lang='java'):\n",
        "    lexer = get_lexer_by_name(lang)\n",
        "    tokens = lex(code, lexer)\n",
        "    token_list = [t[1] for t in tokens if t[0] in Token]\n",
        "    return token_list\n",
        "\n",
        "\n",
        "smooth_fn = SmoothingFunction().method1\n",
        "bleu_scores = {'BLEU-1': [], 'BLEU-2': [], 'BLEU-3': [], 'BLEU-4': []}\n",
        "\n",
        "for ref, pred in zip(references, predictions):\n",
        "    ref_tokens = [tokenize_code(ref)]\n",
        "    pred_tokens = tokenize_code(pred)\n",
        "\n",
        "    bleu1 = sentence_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "    bleu_scores['BLEU-1'].append(bleu1)\n",
        "    bleu_scores['BLEU-2'].append(bleu2)\n",
        "    bleu_scores['BLEU-3'].append(bleu3)\n",
        "    bleu_scores['BLEU-4'].append(bleu4)\n",
        "\n",
        "print(\"==== BLEU Scores ====\")\n",
        "for key in bleu_scores:\n",
        "    average_score = sum(bleu_scores[key]) / len(bleu_scores[key])\n",
        "    print(f'{key}: {average_score:.4f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGUPM_5jUMN1",
        "outputId": "70236b3a-c1ee-407d-aacc-4033f68e8279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "METEOR: 0.9103\n"
          ]
        }
      ],
      "source": [
        "#METEOR 점수 정\n",
        "def tokenize_code(code):\n",
        "    return nltk.word_tokenize(' '.join(code.rstrip('\\n')[0:-1].split()).strip('\"'))\n",
        "\n",
        "meteor_scores = []\n",
        "meteor_s=0.0\n",
        "for ref, pred in zip(references, predictions):\n",
        "    ref_tokens = tokenize_code(ref)\n",
        "    pred_tokens = tokenize_code(pred)\n",
        "    #score = nltk.translate.meteor_score.meteor_score([ref], pred )\n",
        "    score = nltk.translate.meteor_score.meteor_score([ref_tokens], pred_tokens )\n",
        "    #meteor_scores.append(score)\n",
        "    meteor_s += score\n",
        "\n",
        "average_meteor = meteor_s / len(predictions)\n",
        "print(f'\\nMETEOR: {average_meteor:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARYJ2ny2UUtz",
        "outputId": "0a65867c-85ac-46e9-ab3f-d46c72a7f3ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using default tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== ROUGE-LCS Scores ====\n",
            "Precision: 0.9140\n",
            "Recall: 0.9207\n",
            "F-measure: 0.9142\n"
          ]
        }
      ],
      "source": [
        "#ROUGE-LCS Score 측정\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
        "\n",
        "rouge_precision = []\n",
        "rouge_recall = []\n",
        "rouge_fmeasure = []\n",
        "\n",
        "for ref, pred in zip(references, predictions):\n",
        "    scores = scorer.score(pred, ref)\n",
        "    rouge_l = scores['rougeL']\n",
        "    rouge_precision.append(rouge_l.precision)\n",
        "    rouge_recall.append(rouge_l.recall)\n",
        "    rouge_fmeasure.append(rouge_l.fmeasure)\n",
        "\n",
        "average_precision = sum(rouge_precision) / len(rouge_precision)\n",
        "average_recall = sum(rouge_recall) / len(rouge_recall)\n",
        "average_fmeasure = sum(rouge_fmeasure) / len(rouge_fmeasure)\n",
        "\n",
        "print(\"\\n==== ROUGE-LCS Scores ====\")\n",
        "print(f'Precision: {average_precision:.4f}')\n",
        "print(f'Recall: {average_recall:.4f}')\n",
        "print(f'F-measure: {average_fmeasure:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d3zwu2MUVTQ",
        "outputId": "2f944ad9-57a1-40a2-a063-c9aa00d6ee50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== Summary ====\n",
            "BLEU-1: 0.9123\n",
            "BLEU-2: 0.8997\n",
            "BLEU-3: 0.8893\n",
            "BLEU-4: 0.8783\n",
            "METEOR: 0.9123\n",
            "ROUGE-L Precision: 0.9207\n",
            "ROUGE-L Recall: 0.9140\n",
            "ROUGE-L F-measure: 0.9142\n",
            "예측값과 레퍼런스가 동일한 개수: 17174/61258\n"
          ]
        }
      ],
      "source": [
        "lang = 'java'\n",
        "import CodeBLEU_2\n",
        "\n",
        "print(\"\\n==== Summary ====\")\n",
        "for key in bleu_scores:\n",
        "    average_score = sum(bleu_scores[key]) / len(bleu_scores[key])\n",
        "    print(f'{key}: {average_score:.4f}')\n",
        "print(f'METEOR: {average_meteor:.4f}')\n",
        "print(f'ROUGE-L Precision: {average_precision:.4f}')\n",
        "print(f'ROUGE-L Recall: {average_recall:.4f}')\n",
        "print(f'ROUGE-L F-measure: {average_fmeasure:.4f}')\n",
        "#print(f'CodeBLEU: {codebleu_score:.4f}')\n",
        "\n",
        "\n",
        "identical_count = sum([1 for ref, pred in zip(references, predictions) if ref == pred])\n",
        "total_count = len(references)\n",
        "\n",
        "print(f\"예측값과 레퍼런스가 동일한 개수: {identical_count}/{total_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCmKWgIg8pYc",
        "outputId": "be79714d-b35f-4c02-bbfd-2699b9cf98a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: javalang in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from javalang) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install javalang\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "import subprocess\n",
        "import re\n",
        "import javalang\n",
        "from typing import List, Dict\n",
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc-qD5Na8lvn"
      },
      "outputs": [],
      "source": [
        "# try-catch, system call, api 추출 방법\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "system_call_variables=set()\n",
        "API_PACKAGES = {'org', 'com', 'net'}\n",
        "EXCLUDED_LIBRARIES = {'junit','slf4j', 'log4j', 'logback','mavenlogger','log','logoutputstream','logger'} #,'org.apache.maven'\n",
        "\n",
        "\n",
        "def extract_variables(line, id):\n",
        "    lines = line.strip()\n",
        "    if not lines:\n",
        "        return None, None\n",
        "\n",
        "    if lines.startswith('//') or lines.startswith('/*') or lines.startswith('*') or lines.endswith('*/'):\n",
        "        return None,None\n",
        "\n",
        "    try:\n",
        "        tokens = list(javalang.tokenizer.tokenize(lines))\n",
        "        if not tokens:\n",
        "            return None, None\n",
        "\n",
        "        parser = javalang.parser.Parser(tokens)\n",
        "        node = None\n",
        "        try:\n",
        "            node = parser.parse_statement()\n",
        "            #node = parser.parse_local_variable_declaration_statement()\n",
        "        except:\n",
        "            parser = javalang.parser.Parser(tokens)\n",
        "            node = parser.parse_member_declaration()\n",
        "\n",
        "        if node is None:\n",
        "            return None, None\n",
        "\n",
        "        if isinstance(node, (javalang.tree.VariableDeclaration, javalang.tree.FieldDeclaration)):\n",
        "            type_node = node.type\n",
        "            #if isinstance(type_node, javalang.tree.ReferenceType):\n",
        "            type_name = type_node.name\n",
        "            print(type_name, id_list)\n",
        "            if type_name in id_list:\n",
        "                for declarator in node.declarators:\n",
        "                    variable_name = declarator.name\n",
        "                    print(f\"Extracted variable: {variable_name} of type {type_name}\")\n",
        "                    return type_name, variable_name\n",
        "\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    except (javalang.parser.JavaSyntaxError, javalang.tokenizer.LexerError, IndexError, StopIteration) and Exception as e:\n",
        "        return None, None\n",
        "\n",
        "def is_variable_used(line, variable_name):\n",
        "    usage_pattern = r'\\b{}\\b'.format(re.escape(variable_name))\n",
        "    return re.search(usage_pattern, line) is not None\n",
        "\n",
        "def is_real_system_call(line):\n",
        "    lines = line.strip()\n",
        "    if not lines:\n",
        "        return False\n",
        "\n",
        "    if lines.startswith('//') or lines.startswith('/*') or lines.startswith('*') or lines.endswith('*/'):\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        tokens = list(javalang.tokenizer.tokenize(lines))\n",
        "        if not tokens:\n",
        "            return False\n",
        "\n",
        "        parser = javalang.parser.Parser(tokens)\n",
        "        try:\n",
        "            node = parser.parse_statement()\n",
        "        except:\n",
        "            parser = javalang.parser.Parser(tokens)\n",
        "            #node = parser.parse_expression()\n",
        "            node = parser.parse_member_declaration()\n",
        "\n",
        "        system_call_found = False\n",
        "\n",
        "        def get_type_name(type_node):\n",
        "            if isinstance(type_node, javalang.tree.ReferenceType):\n",
        "                type_name = type_node.name\n",
        "                if type_node.arguments:\n",
        "                    for arg in type_node.arguments:\n",
        "                        if isinstance(arg, javalang.tree.TypeArgument):\n",
        "                            arg_type_name = get_type_name(arg.type)\n",
        "                            if arg_type_name:\n",
        "                                return arg_type_name\n",
        "                return type_name\n",
        "            elif isinstance(type_node, javalang.tree.BasicType):\n",
        "                return type_node.name.lower()\n",
        "            return None\n",
        "\n",
        "        def traverse(node):\n",
        "            nonlocal system_call_found\n",
        "\n",
        "            ignored_node_types = (\n",
        "                javalang.tree.MethodDeclaration,\n",
        "                javalang.tree.ClassDeclaration,\n",
        "                javalang.tree.Literal\n",
        "            )\n",
        "            if isinstance(node, ignored_node_types):\n",
        "                return\n",
        "\n",
        "\n",
        "            if isinstance(node, javalang.tree.FormalParameter):\n",
        "                param_type = get_type_name(node.type)\n",
        "                if param_type and param_type in system_call_patterns_lower:\n",
        "                    variable_name = node.name\n",
        "                    system_call_variables.add(variable_name)\n",
        "                    #system_call_found = True\n",
        "                    return\n",
        "\n",
        "            if isinstance(node, (javalang.tree.VariableDeclaration, javalang.tree.FieldDeclaration)):\n",
        "                type_node = node.type #if hasattr(node, 'type') else None\n",
        "                type_name = type_node.name\n",
        "                if type_name in system_call_patterns:\n",
        "                    for decl in node.declarators:\n",
        "                        variable_name = decl.name\n",
        "                        system_call_variables.add(variable_name)\n",
        "                    system_call_found = True\n",
        "                    return\n",
        "\n",
        "            # if isinstance(node, javalang.tree.MemberReference):\n",
        "            #     variable_name = node.member\n",
        "            #     if variable_name in system_call_variables:\n",
        "            #         system_call_found = True\n",
        "            #         return\n",
        "\n",
        "                    #if type_name in system_call_patterns_lower:\n",
        "                    #    system_call_found = True\n",
        "                    #    return\n",
        "\n",
        "            if isinstance(node, javalang.tree.MethodInvocation):\n",
        "\n",
        "                method_name = node.member.lower()\n",
        "                if method_name in system_call_patterns_lower:\n",
        "                    system_call_found = True\n",
        "                    return\n",
        "\n",
        "            # if isinstance(node, javalang.tree.ClassCreator):\n",
        "            #     if hasattr(node, 'type') and node.type:\n",
        "            #         type_name = node.type.name.lower()\n",
        "            #         if type_name in system_call_patterns_lower:\n",
        "            #             system_call_found = True\n",
        "            #             return\n",
        "\n",
        "            for child in node.children:\n",
        "                if isinstance(child, (list, tuple)):\n",
        "                    for item in child:\n",
        "                        if isinstance(item, javalang.ast.Node):\n",
        "                            traverse(item)\n",
        "                elif isinstance(child, javalang.ast.Node):\n",
        "                    traverse(child)\n",
        "\n",
        "        traverse(node)\n",
        "\n",
        "        return system_call_found\n",
        "\n",
        "    except (javalang.parser.JavaSyntaxError, IndexError, StopIteration, TypeError) and Exception as e:\n",
        "        return False\n",
        "\n",
        "def is_method_declaration(line):\n",
        "    method_decl_patterns = [\"void\", \"public\", \"protected\", \"private\", \"static\", \"final\", \"abstract\", \"synchronized\"]\n",
        "    return any(line.strip().lower().startswith(keyword) for keyword in method_decl_patterns)\n",
        "\n",
        "def is_variable_declaration_or_assignment(line):\n",
        "    var_decl_patterns = [r\"\\bint\\b\", r\"\\bfloat\\b\", r\"\\bdouble\\b\", r\"\\bchar\\b\", r\"\\bstring\\b\", r\"\\bboolean\\b\",\n",
        "                         r\"\\blong\\b\", r\"\\bbyte\\b\", r\"\\bshort\\b\"]\n",
        "    return any(re.search(pattern, line, re.IGNORECASE) for pattern in var_decl_patterns)\n",
        "\n",
        "\n",
        "def extract_and_insert_token( method, api_short_patterns):\n",
        "    modified_methods=[]\n",
        "    system_call_variables.clear()\n",
        "    system_call_found = False\n",
        "    try_catch_found = False\n",
        "    api_found = False\n",
        "    api_variable_found= False\n",
        "    system_call_variable_found = False\n",
        "    check = False\n",
        "    api_variables = []\n",
        "    #system_call_variables=[]\n",
        "    code = method.strip()\n",
        "    if code:\n",
        "        lines = code.splitlines()\n",
        "        print(len(lines))\n",
        "        if len(lines) <= 1:\n",
        "          return 0\n",
        "        modified_lines = []\n",
        "        i = 0\n",
        "\n",
        "        in_multiline_comment = False\n",
        "\n",
        "\n",
        "        for line in lines:\n",
        "            if len(api_short_patterns) > 0:\n",
        "                for api_short in api_short_patterns:\n",
        "                    api_name, variable_name = extract_variables(line, api_short)\n",
        "                    if api_name and variable_name:\n",
        "                        if variable_name not in api_variables:\n",
        "                            api_variables.append(variable_name)\n",
        "\n",
        "            for system_call in system_call_patterns:\n",
        "                sys_name, variable_name = extract_variables(line,system_call)\n",
        "                if sys_name and variable_name:\n",
        "                    system_call_variables.add(variable_name)\n",
        "\n",
        "        while i <len(lines):\n",
        "            line = lines[i]\n",
        "            strip_line = line.strip()\n",
        "\n",
        "            if strip_line.startswith(\"/*\"):\n",
        "                in_multiline_comment = True\n",
        "                modified_lines.append(line)\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if in_multiline_comment:\n",
        "                modified_lines.append(line)\n",
        "                if '*/' in strip_line:\n",
        "                    in_multiline_comment = False\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if strip_line.startswith(\"//\"):\n",
        "                #modified_lines.append(line)\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if \"try\" in strip_line.lower() and \"{\" in strip_line:\n",
        "                if re.search(r'try\\s*{', code) and re.search(r'catch\\s*\\(', code):\n",
        "                    try_catch_found = True\n",
        "\n",
        "            if is_real_system_call(line):\n",
        "                system_call_found = True\n",
        "                while i < len(lines):\n",
        "                    modified_lines.append(lines[i])\n",
        "                    if lines[i].strip().endswith(\";\"):\n",
        "                        i += 1\n",
        "                        modified_lines.append(\"<LOG>\")\n",
        "                        break\n",
        "                    i += 1\n",
        "                continue\n",
        "\n",
        "            if any(api_short in strip_line for api_short in api_short_patterns):\n",
        "                api_found = True\n",
        "                while i < len(lines):\n",
        "                    modified_lines.append(lines[i])\n",
        "                    if lines[i].strip().endswith(\";\"):\n",
        "                        i += 1\n",
        "                        modified_lines.append(\"<LOG>\")\n",
        "                        break\n",
        "                    i += 1\n",
        "                continue\n",
        "\n",
        "            api_variable_used = False\n",
        "            for variable in api_variables:\n",
        "                if is_variable_used(line, variable):\n",
        "                    api_variable_found = True\n",
        "                    modified_lines.append(line)\n",
        "                    modified_lines.append(\"<LOG>\")\n",
        "                    api_variable_used = True\n",
        "                    i += 1\n",
        "                    break\n",
        "            if api_variable_used:\n",
        "                continue\n",
        "\n",
        "            system_call_variable_used = False\n",
        "            for variable in system_call_variables:\n",
        "                if is_variable_used(line, variable):\n",
        "                    system_call_variable_found = True\n",
        "                    modified_lines.append(line)\n",
        "                    modified_lines.append(\"<LOG>\")\n",
        "                    system_call_variable_used = True\n",
        "                    i += 1\n",
        "                    break\n",
        "            if system_call_variable_used:\n",
        "                continue\n",
        "\n",
        "            if \"catch\" in strip_line.lower() and \"{\" in strip_line:\n",
        "                if re.search(r'catch\\s*\\(', strip_line):\n",
        "                    try_catch_found = True\n",
        "                    modified_lines.append(line)\n",
        "                    modified_lines.append(\"<LOG>\")\n",
        "                    i += 1\n",
        "                    continue\n",
        "\n",
        "            modified_lines.append(line)\n",
        "            i+=1\n",
        "\n",
        "        modified_lines = '\\n'.join(modified_lines)\n",
        "        if system_call_found == True or api_found == True or try_catch_found==True:\n",
        "            check = True\n",
        "        method_info = {\n",
        "            \"code\": modified_lines,\n",
        "            \"check\": check,\n",
        "        }\n",
        "        modified_methods.append(method_info)\n",
        "    return modified_methods\n",
        "\n",
        "def format_java_code(code):\n",
        "    formatted_code = \"\"\n",
        "    indent_level = 0\n",
        "\n",
        "    for line in code.replace(\"{\", \"{\\n\").replace(\"}\", \"\\n}\").replace(\";\", \";\\n\").splitlines():\n",
        "        line = line.strip()\n",
        "\n",
        "        if line.startswith(\"}\"):\n",
        "            indent_level -= 1\n",
        "\n",
        "        formatted_code += \"\\t\" * indent_level + line\n",
        "\n",
        "        if line.endswith(\"{\"):\n",
        "            indent_level += 1\n",
        "    return formatted_code\n",
        "\n",
        "def flatten_code(code):\n",
        "    flattened_code = ' '.join(line.strip() for line in code.splitlines())\n",
        "    return flattened_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyEWGdhg8-Ap",
        "outputId": "95182e85-f941-469b-f178-7db93442f050"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/operative_config.gin\n",
            "ERROR:root:Path not found: gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word_fixed/operative_config.gin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "System Call, Try Catch, API Yes\n"
          ]
        }
      ],
      "source": [
        "#from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import tempfile\n",
        "import os\n",
        "import gc\n",
        "output_file1= \"/content/ex1.txt\"\n",
        "output_file2= \"/content/ex2.txt\"\n",
        "output_file3= \"/content/ex3.txt\"\n",
        "\n",
        "def get_prediction_token(input_text, model_dir): #method에 system call, api, try catch가 있는 경우\n",
        "    model = t5.models.MtfModel(FINETUNE_MODEL_DIR, tpu='local',sequence_length={\"inputs\": 1024, \"targets\": 2048}, learning_rate_schedule = learning_rate_scheduler)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8') as tmp_input_file:\n",
        "        tmp_input_file.write(input_text)\n",
        "        input_file_path = tmp_input_file.name\n",
        "\n",
        "    tmp_output_file = tempfile.NamedTemporaryFile(delete=False)\n",
        "    output_file_path = tmp_output_file.name\n",
        "    tmp_output_file.close()\n",
        "    vocabulary = t5.data.get_default_vocabulary()\n",
        "    model.predict(input_file=input_file_path, output_file=output_file1, vocabulary= load_vocabulary(),)\n",
        "\n",
        "    with open('ex1.txt-600000', 'r', encoding='utf-8') as f:\n",
        "        prediction = f.read().strip()\n",
        "        print(f.read())\n",
        "    os.remove(input_file_path)\n",
        "    #os.remove('/content/ex1.txt-600000')\n",
        "    return prediction\n",
        "\n",
        "def get_prediction_location(input_text, model_dir): #method에 system call, api, try catch가 없는 경우 로그 삽입 여부 결정\n",
        "    model = t5.models.MtfModel(model_dir, tpu='local',sequence_length={\"inputs\": 1024, \"targets\": 2048}, learning_rate_schedule = learning_rate_scheduler)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8') as tmp_input_file:\n",
        "        tmp_input_file.write(input_text)\n",
        "        input_file_path = tmp_input_file.name\n",
        "\n",
        "    # Generate the prediction\n",
        "    vocabulary = t5.data.get_default_vocabulary()\n",
        "    model.predict(input_file=input_file_path, output_file=output_file2, vocabulary= load_vocabulary())\n",
        "    #output_file= \"/content/ex1.txt-600000\"\n",
        "    with open('/content/ex2.txt-330000', 'r') as f:#, encoding='utf-8') as f:\n",
        "        prediction = f.read().strip()\n",
        "        print(f.read())\n",
        "\n",
        "    os.remove(input_file_path)\n",
        "    #os.remove('/content/ex2.txt-330000')\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def get_prediction_no_token(input_text, model_dir):#method에 system call, api, try catch가 없는 경우 로그 메세지 생성\n",
        "    model = t5.models.MtfModel(model_dir, tpu='local',sequence_length={\"inputs\": 1024, \"targets\": 2048}, learning_rate_schedule = learning_rate_scheduler)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8') as tmp_input_file:\n",
        "        tmp_input_file.write(input_text)\n",
        "        input_file_path = tmp_input_file.name\n",
        "\n",
        "    # Generate the prediction\n",
        "    vocabulary = t5.data.get_default_vocabulary()\n",
        "    model.predict(input_file=input_file_path, output_file=output_file3, vocabulary= load_vocabulary())\n",
        "    #output_file= \"/content/ex1.txt-600000\"\n",
        "    with open('/content/ex3.txt-600000', 'r') as f:#, encoding='utf-8') as f:\n",
        "        prediction = f.read().strip()\n",
        "        print(f.read())\n",
        "\n",
        "    os.remove(input_file_path)\n",
        "    #os.remove('/content/ex3.txt-600000')\n",
        "\n",
        "    return prediction\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    modified_methods = extract_and_insert_token(method, api_short_patterns)\n",
        "    if modified_methods[0]['check'] == True:\n",
        "      print(\"System Call, Try Catch, API Yes\")\n",
        "      input_method = flatten_code(modified_methods[0]['code'])\n",
        "      prediction = get_prediction_token(input_method, FINETUNE_MODEL_DIR)\n",
        "      print(\"Original Input: \", format_java_code(method))\n",
        "      print(\"Preprocess Input:\", format_java_code(input_method))\n",
        "      print(\"Prediction:\", format_java_code(prediction))\n",
        "    else:\n",
        "      print(\"System Call, Try Catch, API No\")\n",
        "      input_method = method\n",
        "      model_directory = f'gs://log_gen/finetuned-model/classifier/polynomial2'\n",
        "      prediction_whether_log_need = get_prediction_location(input_method, model_directory)\n",
        "      print(prediction_whether_log_need)\n",
        "      if prediction_whether_log_need == \"Need\":\n",
        "        print(\"Log Need\")\n",
        "        model_directory2 = f'gs://log_gen/finetuned-model/multi-log-injection/one-to-n/custom/word'\n",
        "        prediction = get_prediction_no_token(input_method, model_directory2)\n",
        "        print(\"Input:\", format_java_code(input_method))\n",
        "        print(\"Prediction:\", format_java_code(prediction))\n",
        "      else:\n",
        "        print(\"Log Not Need\")\n",
        "        print(format_java_code(input_method))\n",
        "\n",
        "\n",
        "    del model\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOqVDi17QBbi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q5lb6RR3uF4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
